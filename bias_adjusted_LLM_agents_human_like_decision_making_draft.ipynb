{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6nJ889yY8El"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# Bias-Adjusted LLM Agents for Human-Like Decision-Making\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025596?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=flat&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n",
        "[![OpenAI](https://img.shields.io/badge/OpenAI-412991.svg?style=flat&logo=openai&logoColor=white)](https://openai.com/)\n",
        "[![Google Cloud](https://img.shields.io/badge/Google_Cloud-4285F4?style=flat&logo=google-cloud&logoColor=white)](https://cloud.google.com/vertex-ai)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2508.18600-b31b1b.svg)](https://arxiv.org/abs/2508.18600v1)\n",
        "[![Research](https://img.shields.io/badge/Research-Computational%20Social%20Science-green)](https://github.com/chirindaopensource/bias_adjusted_LLM_agents_human_like_decision_making)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Behavioral%20Economics%20%26%20NLP-blue)](https://github.com/chirindaopensource/bias_adjusted_LLM_agents_human_like_decision_making)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Agent--Based%20Modeling-orange)](https://github.com/chirindaopensource/bias_adjusted_LLM_agents_human_like_decision_making)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/bias_adjusted_LLM_agents_human_like_decision_making)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/bias_adjusted_LLM_agents_human_like_decision_making`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics\"** by:\n",
        "\n",
        "*   Ayato Kitadai\n",
        "*   Yusuke Fukasawa\n",
        "*   Nariaki Nishino\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for simulating heterogeneous populations of human-like agents in economic games. It implements the paper's novel persona-based approach to condition Large Language Models (LLMs) with empirical, individual-level behavioral data, thereby adjusting their intrinsic biases to better align with observed human decision-making patterns. The goal is to provide a transparent, robust, and extensible toolkit for researchers to replicate, validate, and build upon the paper's findings in the domain of computational social science and agent-based modeling.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: run_entire_project](#key-callable-run_entire_project)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics.\" The core of this repository is the iPython Notebook `bias_adjusted_LLM_agents_human_like_decision_making_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final execution of a full suite of robustness checks.\n",
        "\n",
        "The use of LLMs to simulate human behavior is a promising but challenging frontier. Off-the-shelf models often exhibit behavior that is too rational or systematically biased in ways that do not reflect the diversity of a real human population. This project implements the paper's innovative solution: injecting individual-level behavioral traits from the Econographics dataset into LLMs to create a heterogeneous population of \"digital twins.\"\n",
        "\n",
        "This codebase enables users to:\n",
        "-   Rigorously validate and cleanse the input persona and benchmark experimental data.\n",
        "-   Set up a robust, multi-provider infrastructure for interacting with leading LLMs (OpenAI, Google, Anthropic).\n",
        "-   Execute the full 3x3x2 factorial simulation of the Ultimatum Game with 1,000 agents per condition.\n",
        "-   Leverage a fault-tolerant simulation engine with checkpointing and resume capabilities.\n",
        "-   Aggregate individual agent decisions into population-level probability distributions.\n",
        "-   Quantitatively validate simulation results against empirical data using the Wasserstein distance.\n",
        "-   Generate high-fidelity replications of the paper's key figures and tables.\n",
        "-   Conduct a comprehensive suite of robustness checks to test the stability of the findings.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in behavioral economics, game theory, and natural language processing.\n",
        "\n",
        "**1. The Ultimatum Game:**\n",
        "The experimental testbed is the classic Ultimatum Game. A Proposer offers a split of 100 coins, and a Responder can either accept or reject. The payoff `(u_P, u_R)` is:\n",
        "\n",
        "$$\n",
        "(u_P, u_R) =\n",
        "\\begin{cases}\n",
        "(100 - s, s) & \\text{if Responder accepts} \\\\\n",
        "(0, 0) & \\text{if Responder rejects}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where `s` is the amount offered to the Responder. While classical game theory predicts a minimal offer, human behavior systematically deviates, showing preferences for fairness. This well-documented gap makes it an ideal benchmark for testing the human-likeness of LLM agents.\n",
        "\n",
        "**2. Persona-Based Conditioning:**\n",
        "The core innovation is to move beyond generic LLM agents by providing them with a \"persona\" in their context window. This is achieved by constructing a text block from the **Econographics dataset**, which contains 21 measured behavioral indicators (e.g., Risk Aversion, Reciprocity, Patience) for 1,000 individuals. By assigning each LLM agent a unique persona from this dataset, the simulation aims to create a population whose distribution of behavioral biases mirrors the human sample.\n",
        "\n",
        "**3. Wasserstein Distance for Validation:**\n",
        "The alignment between the simulated distribution of offers (`P_sim`) and the empirical human distribution (`P_human`) is quantified using the Wasserstein-1 distance. For 1D discrete distributions, this is the sum of the absolute differences between the cumulative distribution functions (CDFs):\n",
        "\n",
        "$$\n",
        "W_1(P_{\\text{sim}}, P_{\\text{human}}) = \\sum_{k=0}^{100} |\\text{CDF}_{\\text{sim}}(k) - \\text{CDF}_{\\text{human}}(k)|\n",
        "$$\n",
        "\n",
        "A lower distance indicates a better alignment between the simulated and real-world behavior.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`bias_adjusted_LLM_agents_human_like_decision_making_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Task-Based Architecture:** The entire pipeline is broken down into 8 distinct, modular tasks, from data validation to robustness analysis.\n",
        "-   **Professional-Grade Data Validation:** A comprehensive validation suite ensures all inputs (data and configurations) conform to the required schema before execution.\n",
        "-   **Auditable Data Cleansing:** A non-destructive cleansing process that handles missing values and outliers, returning a detailed log of all transformations.\n",
        "-   **Multi-Provider LLM Infrastructure:** A robust, fault-tolerant system for interacting with OpenAI, Google (Gemini), and Anthropic (Claude) APIs, featuring exponential backoff retries and fallback parsing.\n",
        "-   **Resumable Simulation Engine:** A checkpointing system allows the 18,000-decision simulation to be stopped and resumed without loss of progress.\n",
        "-   **Rigorous Statistical Analysis:** Implements correct aggregation of individual decisions into statistical distributions and applies the Wasserstein distance and alternative metrics (KL, JS Divergence).\n",
        "-   **High-Fidelity Visualization:** Generates publication-quality replications of the paper's main figures.\n",
        "-   **Comprehensive Robustness Suite:** A meta-orchestrator to systematically test the sensitivity of the results to changes in sample size, persona definitions, and prompt wording.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Input Data Validation (Task 1):** Ingests and rigorously validates all raw data and configuration files.\n",
        "2.  **Data Preprocessing (Task 2):** Cleanses and aligns the persona and benchmark datasets.\n",
        "3.  **LLM Infrastructure Setup (Task 3):** Authenticates and initializes clients for all LLM providers.\n",
        "4.  **Simulation Execution (Task 4):** Runs the full 3x3x2 factorial simulation of the Ultimatum Game.\n",
        "5.  **Data Aggregation (Task 5):** Transforms raw decisions into population-level statistical distributions.\n",
        "6.  **Quantitative Validation (Task 6):** Computes Wasserstein distances to measure alignment with human data.\n",
        "7.  **Results Visualization (Task 7):** Generates high-fidelity replications of the paper's figures and tables.\n",
        "8.  **Robustness Analysis (Task 8):** Provides a master function to run the full suite of robustness checks.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `bias_adjusted_LLM_agents_human_like_decision_making_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 8 major tasks.\n",
        "\n",
        "## Key Callable: run_entire_project\n",
        "\n",
        "The central function in this project is `run_entire_project`. It orchestrates the entire analytical workflow, providing a single entry point for running the baseline study replication and the advanced robustness checks.\n",
        "\n",
        "```python\n",
        "def run_entire_project(\n",
        "    personas_data_path: str,\n",
        "    benchmark_data_path: str,\n",
        "    study_configurations: Dict[str, Any],\n",
        "    output_dir: str,\n",
        "    force_rerun_simulation: bool = False,\n",
        "    run_robustness_checks: bool = True,\n",
        "    # ... other robustness configurations\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire research project, including the main study pipeline\n",
        "    and all subsequent robustness analyses.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   API keys for OpenAI, Google Cloud Platform, and access to Anthropic models on Vertex AI.\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `matplotlib`, `tqdm`, `openai`, `google-cloud-aiplatform`, `anthropic`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/bias_adjusted_LLM_agents_human_like_decision_making.git\n",
        "    cd bias_adjusted_LLM_agents_human_like_decision_making\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy matplotlib tqdm openai \"google-cloud-aiplatform>=1.38.1\" \"anthropic[vertex]\" ipython\n",
        "    ```\n",
        "\n",
        "4.  **Set up API Credentials:**\n",
        "    -   Export your API keys as environment variables.\n",
        "        ```sh\n",
        "        export OPENAI_API_KEY=\"sk-...\"\n",
        "        export GCP_PROJECT=\"your-gcp-project-id\"\n",
        "        ```\n",
        "    -   Authenticate with Google Cloud for Vertex AI access.\n",
        "        ```sh\n",
        "        gcloud auth application-default login\n",
        "        ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two CSV files with specific structures, which are rigorously validated by the first task.\n",
        "1.  **Personas Data (`personas.csv`):** A CSV with 1000 rows and 26 columns: `participant_id`, 21 specific behavioral indicators (e.g., `Reciprocity:High`), `age`, `gender`, `country_of_residence`, and `crt_score`.\n",
        "2.  **Benchmark Data (`benchmark.csv`):** A CSV with 1000 rows and 3 columns: `interaction_id`, `proposer_offer`, and `responder_decision`.\n",
        "\n",
        "A mock data generation script is provided in the main notebook to create valid example files for testing the pipeline.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `bias_adjusted_LLM_agents_human_like_decision_making_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Create your data CSVs or use the provided mock data generator. Ensure your API keys are set as environment variables.\n",
        "2.  **Execute Pipeline:** Call the grand master orchestrator function.\n",
        "\n",
        "    ```python\n",
        "    # This single call runs the baseline analysis and all configured robustness checks.\n",
        "    final_project_artifacts = run_entire_project(\n",
        "        personas_data_path=\"./data/personas.csv\",\n",
        "        benchmark_data_path=\"./data/benchmark.csv\",\n",
        "        study_configurations=study_configurations,\n",
        "        output_dir=\"./project_outputs\",\n",
        "        force_rerun_simulation=False,\n",
        "        run_robustness_checks=True,\n",
        "        sample_sizes_to_test=[500, 750]\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** Programmatically access any result from the returned dictionary. For example, to view the main results table:\n",
        "    ```python\n",
        "    main_results_table = final_project_artifacts['main_study_results']['results_table_data']\n",
        "    print(main_results_table)\n",
        "    ```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `run_entire_project` function returns a single, comprehensive dictionary with two top-level keys:\n",
        "-   `main_study_results`: A dictionary containing all artifacts from the primary study replication (cleansed data, logs, raw simulation results, aggregated distributions, final tables, etc.).\n",
        "-   `robustness_analysis_results`: A dictionary containing the summary tables from each of the executed robustness checks.\n",
        "\n",
        "All generated files (checkpoints, plots) are saved to the specified `output_dir`.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "bias_adjusted_LLM_agents_human_like_decision_making/\n",
        "│\n",
        "├── bias_adjusted_LLM_agents_human_like_decision_making_draft.ipynb  # Main implementation notebook\n",
        "├── requirements.txt                                                 # Python package dependencies\n",
        "├── LICENSE                                                          # MIT license file\n",
        "└── README.md                                                        # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `study_configurations` dictionary and the arguments to the `run_entire_project` function. Users can easily modify all relevant parameters, such as the list of models to test, the definitions of persona configurations, prompt templates, and the specific robustness checks to perform.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "\n",
        "-   **Automated Report Generation:** Creating a function that takes the final `project_artifacts` dictionary and generates a full PDF or HTML report summarizing the findings, including tables, figures, and interpretive text.\n",
        "-   **Generalization to Other Games:** Refactoring the game-specific logic (e.g., prompt templates, decision validation) to allow the pipeline to be easily adapted to other classic economic games like the Prisoner's Dilemma or Public Goods Game.\n",
        "-   **Advanced Persona Engineering:** Implementing more sophisticated methods for selecting or weighting behavioral traits, potentially using feature selection algorithms to identify the most impactful traits for aligning agent behavior.\n",
        "-   **Formal Scientific Integrity Report:** Implementing functions to perform and document a systematic bias analysis and generate a comprehensive limitations section.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{kitadai2025bias,\n",
        "  title={Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics},\n",
        "  author={Kitadai, Ayato and Fukasawa, Yusuke and Nishino, Nariaki},\n",
        "  journal={arXiv preprint arXiv:2508.18600},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of \"Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics\".\n",
        "GitHub repository: https://github.com/chirindaopensource/bias_adjusted_LLM_agents_human_like_decision_making\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Ayato Kitadai, Yusuke Fukasawa, and Nariaki Nishino for their innovative and clearly articulated research.\n",
        "-   Thanks to the developers of the scientific Python ecosystem (`numpy`, `pandas`, `scipy`, `matplotlib`, etc.) and the teams at OpenAI, Google, and Anthropic for their powerful open-source tools and proprietary models.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `bias_adjusted_LLM_agents_human_like_decision_making_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "VsYrbdx7LbWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics*\"\n",
        "\n",
        "Authors: Ayato Kitadai, Yusuke Fukasawa, Nariaki Nishino\n",
        "\n",
        "E-Journal Submission Date: 26 August 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2508.18600v1\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Large language models (LLMs) are increasingly used to simulate human decision-making, but their intrinsic biases often diverge from real human behavior--limiting their ability to reflect population-level diversity. We address this challenge with a persona-based approach that leverages individual-level behavioral data from behavioral economics to adjust model biases. Applying this method to the ultimatum game--a standard but difficult benchmark for LLMs--we observe improved alignment between simulated and empirical behavior, particularly on the responder side. While further refinement of trait representations is needed, our results demonstrate the promise of persona-conditioned LLMs for simulating human-like decision patterns at scale."
      ],
      "metadata": {
        "id": "O2U8Wf6oZbzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **The Core Problem and Motivation**\n",
        "\n",
        "The paper begins by identifying a fundamental challenge in using Large Language Models (LLMs) to simulate human behavior. While LLMs are powerful, they are not \"blank slates.\" They come with intrinsic biases inherited from their training data and architecture. Consequently, when used as \"agents\" in simulations, their decisions often deviate systematically from those of actual humans. This limits their utility as substitutes for human subjects in economic or social science research, as they fail to capture the heterogeneity and well-documented \"irrationalities\" of human populations.\n",
        "\n",
        "The authors frame this as a gap between the *normative* (or purely rational) behavior often exhibited by LLMs and the *descriptive* reality of human decision-making, a gap that behavioral economics has spent decades exploring.\n",
        "\n",
        "### **The Proposed Solution - A Persona-Based Approach**\n",
        "\n",
        "The central contribution of this paper is its method for \"bias adjustment.\" Instead of accepting the LLM's default behavior, the authors propose conditioning the model on a rich, empirically-grounded \"persona.\"\n",
        "\n",
        "Their solution involves three key stages, as illustrated in their Figure 1:\n",
        "\n",
        "1.  **Measure:** They leverage a real-world dataset called **Econographics** (Chapman et al., 2022). This dataset contains measurements of 21 distinct behavioral economic indicators (e.g., risk aversion, inequality aversion, reciprocity, patience) from 1,000 diverse human participants in the U.S.\n",
        "2.  **Inject:** They translate the individual-level data for each of the 1,000 participants into a detailed persona description. This description is then injected directly into the LLM's prompt, instructing the model to \"embody a character with the following personality traits and demographics.\"\n",
        "3.  **Simulate:** With these personas assigned, they run a classic economic experiment and compare the LLM agents' aggregate behavior to the results from human experiments.\n",
        "\n",
        "This approach is powerful because it attempts to construct a *heterogeneous population* of agents that mirrors the distribution of behavioral traits in a real human sample, rather than relying on a single, monolithic \"LLM personality.\"\n",
        "\n",
        "### **The Experimental Testbed - The Ultimatum Game**\n",
        "\n",
        "To test their method, the authors wisely choose the **Ultimatum Game**, a cornerstone of experimental economics.\n",
        "\n",
        "*   **The Game:** A \"Proposer\" is given 100 coins and must offer a split to a \"Responder.\" The Responder can either **accept** the offer (and the coins are split as proposed) or **reject** it (in which case both players get nothing).\n",
        "*   **Why it's a good test:** Classical game theory predicts the Proposer should offer the smallest possible non-zero amount (e.g., 1 coin) and the Responder should always accept. However, decades of human experiments show that Proposers typically offer around 40-50 coins, and Responders frequently reject \"unfair\" offers (e.g., below 20-30 coins), demonstrating a preference for fairness and a willingness to punish unfairness. This stark divergence between theory and reality makes it an excellent benchmark for testing an agent's \"human-likeness.\"\n",
        "\n",
        "The authors use experimental data from **Lin et al. (2020)** as their ground-truth benchmark for human behavior in this game.\n",
        "\n",
        "### **Experimental Design and Quantitative Evaluation**\n",
        "\n",
        "The methodology is rigorous. The authors test three leading LLMs (GPT-4o, Claude 3 Sonnet, Gemini 1.5 Pro) under three conditions:\n",
        "\n",
        "1.  **Baseline:** No persona is provided.\n",
        "2.  **6-Trait Persona:** Each agent is given a persona based on the six most important behavioral indicators identified via Principal Component Analysis (PCA) in the original Econographics study.\n",
        "3.  **21-Trait Persona:** Each agent is given the full persona with all 21 measured behavioral indicators.\n",
        "\n",
        "To quantify the alignment between the simulated and human distributions, they use the **Wasserstein distance** (also known as the Earth Mover's Distance). This is an excellent choice of metric. Unlike simpler metrics like KL-divergence, it is well-suited for comparing distributions over a metric space (like the number of coins offered) and provides a more intuitive measure of the \"cost\" of transforming one distribution into another. A lower Wasserstein distance signifies a better match.\n",
        "\n",
        "### **Key Findings and Results**\n",
        "\n",
        "The results are both compelling and nuanced, broken down by the two roles in the game.\n",
        "\n",
        "*   **Responder Behavior (The Big Success):**\n",
        "    *   *Without personas*, all LLMs behaved like hyper-rational agents, accepting almost any offer above a very low threshold. This resulted in a flat acceptance curve, completely unlike the human data (Figure 4, orange bubbles).\n",
        "    *   *With personas*, the results improved dramatically. The LLM agents began rejecting unfair offers, producing a monotonically increasing acceptance curve that closely mirrors the human pattern (Figure 4, blue and green bubbles). The Wasserstein distance for the responder side saw a substantial reduction across all models (Table 2), with GPT-4o showing an impressive drop from 0.289 to 0.090.\n",
        "    *   **My Interpretation:** This demonstrates that traits like \"inequality aversion\" and \"reciprocity\" from the Econographics dataset can be effectively injected into LLMs to replicate the fairness-driven, reactive decisions of human responders.\n",
        "\n",
        "*   **Proposer Behavior (More Modest Improvement):**\n",
        "    *   *Without personas*, the models exhibited strong, idiosyncratic biases. For example, GPT-4o and Gemini strongly preferred offering exactly 40 coins, while Claude preferred 50 (Figure 3).\n",
        "    *   *With personas*, the behavior shifted to be more human-like (e.g., the mode shifted towards 50), but the improvement was less pronounced than on the responder side. The distributions remained too narrow and failed to capture the full variance of human offers. The reduction in Wasserstein distance was modest (Table 2).\n",
        "    *   **My Interpretation:** The proposer's task is cognitively more demanding. It requires not just an internal preference for fairness but also a \"theory of mind\"—an anticipation of how the responder will react. The static behavioral traits, while helpful, may be insufficient to fully steer this more complex, strategic reasoning process.\n",
        "\n",
        "### **Conclusion and Critical Assessment**\n",
        "\n",
        "The paper concludes that persona-conditioning using empirical behavioral data is a promising technique for creating more realistic LLM agents, particularly for decisions involving fairness and social preferences.\n",
        "\n",
        "As a professor, here is my critical assessment:\n",
        "\n",
        "*   **Strengths:**\n",
        "    *   **Methodological Rigor:** The use of a real-world behavioral dataset (Econographics), a classic experimental game, and a proper distributional metric (Wasserstein distance) makes the study robust.\n",
        "    *   **Clear Contribution:** It provides a clear, replicable proof-of-concept for bridging the gap between behavioral economics and AI agent simulation.\n",
        "    *   **Nuanced Analysis:** The authors correctly identify and discuss the differential impact of their method on the proposer and responder roles, which points to important avenues for future research.\n",
        "\n",
        "*   **Limitations and Future Directions:**\n",
        "    *   **Internal Validity:** The authors rightly note a key limitation: the participants in the Econographics dataset are not the same as those in the Lin et al. ultimatum game experiment. A gold-standard study would collect both behavioral indicators and game decisions from the same cohort.\n",
        "    *   **External Validity:** The study is confined to a single, one-shot game. The real test will be to see if this method generalizes to more complex, dynamic, and interactive scenarios (e.g., public goods games, repeated prisoner's dilemmas, or market simulations).\n",
        "    *   **The Proposer Puzzle:** The remaining discrepancy in proposer behavior is the most interesting theoretical challenge. It suggests that future work may need to incorporate more than just static traits, perhaps by modeling cognitive processes, social learning, or more dynamic aspects of personality.\n",
        "    *   **Alternative Methods:** The paper focuses on prompting. A comparison with fine-tuning LLMs on behavioral data would be a valuable next step to understand the trade-offs between these two approaches.\n",
        "\n",
        "In summary, this is an excellent piece of research. It moves beyond simply showing that LLMs are biased and proposes a concrete, data-driven method to mitigate those biases, pushing us closer to the vision of using LLMs for high-fidelity simulations of human populations. It is a foundational work that opens up many exciting questions for future exploration."
      ],
      "metadata": {
        "id": "ysXSCZNYgB_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "alqDkV1gU0wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Bias-Adjusted LLM Agents for Human-Like Decision-Making via Behavioral Economics\n",
        "#\n",
        "#  This module provides a complete, production-grade replication and extension\n",
        "#  of the analytical framework presented in \"Bias-Adjusted LLM Agents for\n",
        "#  Human-Like Decision-Making via Behavioral Economics\" by Kitadai, Fukasawa,\n",
        "#  and Nishino (2025). It delivers a robust, end-to-end pipeline for simulating\n",
        "#  heterogeneous, human-like agent populations in economic games by conditioning\n",
        "#  Large Language Models (LLMs) with empirical behavioral data.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Persona-based agent conditioning using the Econographics dataset.\n",
        "#  • Factorial simulation design (3 LLMs x 3 Persona Configs x 2 Roles).\n",
        "#  • High-fidelity replication of the one-shot Ultimatum Game.\n",
        "#  • Quantitative validation using Wasserstein distance against empirical data.\n",
        "#  • Comprehensive robustness and sensitivity analysis framework.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Modular, multi-stage pipeline: Validation -> Cleansing -> Simulation -> Analysis.\n",
        "#  • Multi-provider LLM API integration (OpenAI, Google, Anthropic) with\n",
        "#    robust error handling, exponential backoff, and fallback parsing.\n",
        "#  • Fault-tolerant simulation engine with file-based checkpointing and resume\n",
        "#    capabilities for all 18 experimental conditions.\n",
        "#  • Systematic aggregation of micro-level decisions into macro-level\n",
        "#    probability distributions and conditional acceptance rate functions.\n",
        "#  • High-fidelity replication of all tables and figures from the source paper.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Kitadai, A., Fukasawa, Y., & Nishino, N. (2025). Bias-Adjusted LLM Agents\n",
        "#  for Human-Like Decision-Making via Behavioral Economics. arXiv preprint\n",
        "#  arXiv:2508.18600v1.\n",
        "#  https://arxiv.org/abs/2508.18600v1\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# =============================================================================\n",
        "# Standard Library Imports\n",
        "# =============================================================================\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "# =============================================================================\n",
        "# Third-Party Library Imports\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Visualization ---\n",
        "# The core plotting library for generating figures\n",
        "import matplotlib.pyplot as plt\n",
        "# Specific components for advanced plot customization\n",
        "from matplotlib.axes import Axes\n",
        "from matplotlib.figure import Figure\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "# --- Statistical Analysis ---\n",
        "# For calculating Kullback-Leibler divergence\n",
        "from scipy.stats import entropy\n",
        "# For calculating the Wasserstein-1 distance\n",
        "from scipy.stats import wasserstein_distance\n",
        "# For calculating the Jensen-Shannon distance\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "# --- LLM Provider SDKs ---\n",
        "# For interacting with Anthropic models (Claude) on Google Cloud\n",
        "import anthropic\n",
        "# For interacting with Google models (Gemini) on Vertex AI\n",
        "import vertexai\n",
        "# For interacting with OpenAI models (GPT)\n",
        "import openai\n",
        "\n",
        "# --- Interactive Display (Optional but recommended for notebooks) ---\n",
        "# For rendering styled pandas DataFrames in environments like Jupyter\n",
        "from IPython.display import display\n",
        "# The specific type hint for a pandas Styler object\n",
        "from pandas.io.formats.style import Styler\n"
      ],
      "metadata": {
        "id": "8S44yc6XU57o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "0KBDQBzbU9xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Discussion of the Inputs, Processes and Outputs (IPO Analysis) of Key Callables\n",
        "\n",
        "### **Task 1: Data Validation and Quality Assurance**\n",
        "\n",
        "#### **1. `validate_study_inputs` (Orchestrator)**\n",
        "*   **Inputs:** Raw `personas_df` (`pd.DataFrame`), raw `benchmark_df` (`pd.DataFrame`), `study_configurations` (`Dict`).\n",
        "*   **Process:** Sequentially executes a series of internal validation functions (`_validate_configuration_schema`, `_validate_personas_dataframe`, etc.) to perform a comprehensive check on all inputs. It verifies data shapes, types, value ranges, statistical properties, and cross-dataset consistency.\n",
        "*   **Outputs:** Returns `True` if all validations pass. Halts execution by raising a `ValueError` or `KeyError` upon the first detected failure.\n",
        "*   **Data Transformation:** This function is purely diagnostic and performs no data transformation. It reads the inputs and either approves them or rejects them.\n",
        "*   **Role in Research Pipeline:** This function serves as the **Initial Gatekeeper**. It ensures that the raw data and parameters conform to the exact specifications required by the study's methodology *before* any computational resources are expended. It operationalizes the implicit assumptions about the data described throughout Section 2 (\"Methodology\") of the paper.\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 2: Data Preprocessing and Cleansing**\n",
        "\n",
        "#### **2. `preprocess_and_cleanse_data` (Orchestrator)**\n",
        "*   **Inputs:** Validated raw `personas_df` (`pd.DataFrame`), validated raw `benchmark_df` (`pd.DataFrame`), `study_configurations` (`Dict`).\n",
        "*   **Process:** Executes a sequence of cleansing and alignment operations. For `personas_df`, it handles missing values (listwise deletion/imputation), treats outliers (Winsorization), and standardizes data types. For `benchmark_df`, it removes duplicates and invalid entries. Finally, it sorts both DataFrames to enforce a canonical 1-to-1 mapping.\n",
        "*   **Outputs:** A tuple containing the `clean_personas_df` (`pd.DataFrame`), `clean_benchmark_df` (`pd.DataFrame`), and a `cleansing_log` (`Dict`) that provides a complete audit trail of all modifications.\n",
        "*   **Data Transformation:** This function is purely transformational. It takes raw, validated data and transforms it into clean, aligned, analysis-ready data. Key transformations include dropping/filling rows, clipping outlier values, casting data types, and re-ordering rows.\n",
        "*   **Role in Research Pipeline:** This function is the **Data Preparation Engine**. It ensures that the data fed into the simulation is of the highest possible quality and is structured precisely for the 1-to-1 agent-condition pairing, a critical aspect of the experimental design described in Section 2.3 (\"Simulation Procedure\").\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 3: LLM Infrastructure and Authentication Setup**\n",
        "\n",
        "#### **3. `setup_llm_infrastructure` (Orchestrator)**\n",
        "*   **Inputs:** `study_configurations` (`Dict`).\n",
        "*   **Process:** Reads API credential information from the configuration, securely loads credentials from environment variables, and instantiates and authenticates the official Python SDK clients for the three LLM providers (OpenAI, Google Vertex AI, Anthropic on Vertex AI).\n",
        "*   **Outputs:** An `infrastructure` dictionary containing a `client_registry` that maps model identifiers to their respective live, authenticated client objects.\n",
        "*   **Data Transformation:** Transforms configuration data (environment variable names) into active, stateful software objects (API clients).\n",
        "*   **Role in Research Pipeline:** This function is the **API Abstraction Layer**. It handles the complex, provider-specific details of authentication and provides a simple, unified interface for the simulation engine to access the required computational resources (the LLMs).\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 4: Simulation Execution Engine**\n",
        "\n",
        "#### **4. `run_full_simulation` (Orchestrator)**\n",
        "*   **Inputs:** `clean_personas_df` (`pd.DataFrame`), `clean_benchmark_df` (`pd.DataFrame`), `study_configurations` (`Dict`), `infrastructure` (`Dict`), `checkpoint_dir` (`str`), `force_rerun` (`bool`).\n",
        "*   **Process:** Systematically orchestrates the entire 3x3x2 factorial simulation. It iterates through all 18 experimental conditions (3 models x 3 persona configs x 2 roles). For each condition, it intelligently checks for a previously completed result file. If found, it loads the result; otherwise, it calls `_run_single_simulation_condition` to execute the 1,000 agent simulations for that condition, leveraging robust checkpointing.\n",
        "*   **Outputs:** A dictionary, `all_simulation_results`, where keys are `(model, persona, role)` tuples and values are lists of 1,000 result dictionaries, representing every agent decision in the study.\n",
        "*   **Data Transformation:** This is the core generative step of the pipeline. It transforms the prepared data (personas and benchmark conditions) into the primary raw output of the study (simulated decisions) by interacting with the LLMs.\n",
        "*   **Role in Research Pipeline:** This function is the **Experiment Executor**. It operationalizes the entire experimental design described in Section 2.3 (\"Simulation Procedure\"), generating the raw data that forms the basis for all subsequent analysis.\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 5: Data Aggregation and Statistical Processing**\n",
        "\n",
        "#### **5. `aggregate_and_process_data` (Orchestrator)**\n",
        "*   **Inputs:** `all_simulation_results` (`Dict`), `clean_benchmark_df` (`pd.DataFrame`).\n",
        "*   **Process:** Transforms the raw, individual-level simulation results into aggregate statistical objects. For each of the 9 proposer conditions, it computes a Probability Mass Function (PMF). For each of the 9 responder conditions, it computes an acceptance rate function. It applies the identical aggregation logic to the empirical benchmark data.\n",
        "*   **Outputs:** An `analysis_ready_data` dictionary containing the structured statistical distributions for all simulated conditions and the benchmark.\n",
        "*   **Data Transformation:** This function performs a critical data reduction and transformation. It aggregates 18,000 individual data points into a small number of structured statistical objects (PMFs and conditional probability tables).\n",
        "*   **Role in Research Pipeline:** This function is the **Statistical Summarizer**. It prepares the raw simulation output for the quantitative comparison described in Section 3 (\"Results and Discussion\"). It implements the following aggregations:\n",
        "    *   **Proposer PMF:** $P(X=k) = \\frac{\\sum_{i=1}^{1000} \\mathbb{I}(\\text{decision}_i = k)}{1000}$ for $k \\in \\{0, ..., 100\\}$.\n",
        "    *   **Responder Acceptance Rate:** $A(k) = \\frac{\\sum_{i: \\text{offer}_i = k} \\mathbb{I}(\\text{decision}_i = \\text{accept})}{\\sum_{i: \\text{offer}_i = k} 1}$ for each offer level $k$.\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 6: Wasserstein Distance Computation and Validation**\n",
        "\n",
        "#### **6. `compute_and_validate_results` (Orchestrator)**\n",
        "*   **Inputs:** `analysis_ready_data` (`Dict`), `study_configurations` (`Dict`).\n",
        "*   **Process:** Systematically calculates the Wasserstein-1 distance between each of the 18 simulated distributions and its corresponding empirical benchmark distribution. It then formats these numerical results into a structured pandas DataFrame.\n",
        "*   **Outputs:** A raw `pd.DataFrame` containing the final, scaled quantitative results, structured identically to Table 2 in the paper.\n",
        "*   **Data Transformation:** Transforms the aggregated statistical distributions into a final table of scalar distance metrics.\n",
        "*   **Role in Research Pipeline:** This function is the **Quantitative Judgement Engine**. It directly implements the core validation metric of the study, as described in Section 3 (\"Quantitative Evaluation\"). It calculates the Wasserstein-1 distance, which for 1D discrete distributions is given by:\n",
        "    *   $W_1(P, Q) = \\sum_{i=1}^{n} |\\text{CDF}_P(i) - \\text{CDF}_Q(i)|$\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 7: Results Visualization and Documentation**\n",
        "\n",
        "#### **7. `generate_visualizations` (Orchestrator)**\n",
        "*   **Inputs:** `analysis_ready_data` (`Dict`), `study_configurations` (`Dict`), `output_dir` (`str`).\n",
        "*   **Process:** Uses the aggregated statistical data to generate high-fidelity replications of the key figures from the paper. It calls specialized plotting functions to create the multi-panel histogram plot (Figure 3) and the multi-panel bubble plot (Figure 4).\n",
        "*   **Outputs:** Saves image files (`.png`) of the figures to the specified output directory.\n",
        "*   **Data Transformation:** Transforms the numerical statistical objects into graphical representations for qualitative analysis and communication.\n",
        "*   **Role in Research Pipeline:** This function is the **Visual Communication Module**. It is responsible for replicating Figure 3 (\"Distribution of offers proposed by LLM agents\") and Figure 4 (\"Acceptance rates of LLM agents in the responder role\"), which are the primary visual evidence supporting the paper's conclusions.\n",
        "\n",
        "--\n",
        "\n",
        "### **Task 8: Research Pipeline Orchestration and Robustness Analysis**\n",
        "\n",
        "#### **8. `run_comprehensive_robustness_analysis` (Orchestrator)**\n",
        "*   **Inputs:** `analysis_ready_data` (`Dict`), `clean_personas_df` (`pd.DataFrame`), `clean_benchmark_df` (`pd.DataFrame`), `study_configurations` (`Dict`), and various control parameters.\n",
        "*   **Process:** Orchestrates a suite of robustness checks. It conditionally calls `run_sensitivity_analyses` to re-run the entire pipeline with varied inputs (e.g., smaller sample sizes) and `run_alternative_metric_validation` to re-calculate proposer distribution distances using different metrics (KL and JS Divergence).\n",
        "*   **Outputs:** A dictionary containing the structured results of all robustness analyses performed.\n",
        "*   **Data Transformation:** This function does not transform data itself but orchestrates other functions that do, collecting their results into a final summary object.\n",
        "*   **Role in Research Pipeline:** This function is the **Robustness and Validation Suite**. It addresses the critical scientific need to test the stability and reliability of the main findings, ensuring they are not artifacts of specific methodological choices (like sample size or distance metric).\n",
        "\n",
        "#### **9. `execute_bias_adjusted_llm_study` (Master Orchestrator)**\n",
        "*   **Inputs:** Flexible inputs, accepting either file paths to raw data or pre-loaded pandas DataFrames, along with the `study_configurations` and an `output_dir`.\n",
        "*   **Process:** Executes the entire research pipeline in a single, sequential workflow. It calls the orchestrator for each major task (Validate -> Cleanse -> Setup -> Simulate -> Aggregate -> Quantify -> Visualize) in the correct order, passing the artifacts from one stage to the next.\n",
        "*   **Outputs:** A comprehensive dictionary, `pipeline_artifacts`, containing the key outputs from every stage of the pipeline, providing a complete, auditable record of the entire run.\n",
        "*   **Data Transformation:** This function orchestrates the entire chain of data transformations, from raw CSV files to final plots and tables.\n",
        "*   **Role in Research Pipeline:** This is the **Master Controller**. It encapsulates the entire research methodology of the paper into a single, executable command, ensuring perfect reproducibility.\n",
        "\n",
        "#### **10. `run_entire_project` (Grand Master Orchestrator)**\n",
        "*   **Inputs:** The highest-level inputs: raw data paths, base configurations, output directory, and control flags.\n",
        "*   **Process:** Serves as the ultimate entry point. It first calls `execute_bias_adjusted_llm_study` to run the main experiment. It then unpacks the necessary artifacts from that run and calls `run_comprehensive_robustness_analysis` to perform the subsequent validation checks.\n",
        "*   **Outputs:** A final, nested dictionary containing the complete results of both the main study and all robustness analyses.\n",
        "*   **Data Transformation:** Orchestrates the entire project, managing the flow of data between the main pipeline and the robustness analysis pipeline.\n",
        "*   **Role in Research Pipeline:** This is the **Project Executor**. It represents the full scientific inquiry, from initial hypothesis testing (the main run) to rigorous self-critique (the robustness checks), all within a single, automated workflow.\n",
        "\n",
        "<br> <br>\n",
        "\n",
        "### Abbreviated Usage Example\n",
        "\n",
        "This example demonstrates how to invoke the grand master orchestrator, `run_entire_project`. It covers the setup of all necessary inputs, including the creation of mock data files that now correctly include all 21 specified behavioral indicators.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 1: Setting Up the Environment and Mock Data (Corrected)**\n",
        "\n",
        "Before we can run the pipeline, we need the input files. We will programmatically create mock versions of these files that conform to the exact structure and data types validated by our pipeline, including the specific names for all 21 behavioral traits mentioned in the paper's methodology.\n",
        "\n",
        "**Python Code Snippet: Creating Mock Data Files (Corrected)**\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the directory to store our example data and outputs.\n",
        "EXAMPLE_DIR = \"./project_run_example\"\n",
        "os.makedirs(EXAMPLE_DIR, exist_ok=True)\n",
        "\n",
        "# --- Create Mock Personas Data (personas.csv) ---\n",
        "print(\"Creating mock personas data file with 21 specific indicators...\")\n",
        "num_participants = 1000\n",
        "\n",
        "# Define the complete list of 21 behavioral indicator columns, as derived\n",
        "# from Table 1 in the paper. This is critical for accurate replication.\n",
        "all_21_indicator_cols = [\n",
        "    'Reciprocity:High', 'Reciprocity:Low', 'Altruism', 'Trust',\n",
        "    'Anti-social Punishment', 'Pro-social Punishment', 'Patience',\n",
        "    'Inequality Aversion / WTP', 'Dislike Having More', 'Dislike Having Less', 'WTP',\n",
        "    'Risk Aversion:Gains', 'Risk Aversion:Losses', 'Risk Aversion:Gain/Loss',\n",
        "    'Risk Aversion:CR (Certain)', 'Risk Aversion:CR (Lottery)', 'WTA',\n",
        "    'Ambiguity Aversion', 'Compound Lottery Aversion',\n",
        "    'Overestimation', 'Overplacement', 'Overprecision'\n",
        "]\n",
        "\n",
        "# Generate random standardized data (mean=0, std=1) for all 21 traits.\n",
        "persona_data = {col: np.random.randn(num_participants) for col in all_21_indicator_cols}\n",
        "\n",
        "# Add participant IDs.\n",
        "persona_data['participant_id'] = np.arange(1, num_participants + 1)\n",
        "\n",
        "# Add demographic data with the correct formats.\n",
        "persona_data['age'] = np.random.randint(18, 75, size=num_participants)\n",
        "persona_data['gender'] = np.random.choice(['male', 'female'], size=num_participants)\n",
        "persona_data['country_of_residence'] = 'US'\n",
        "persona_data['crt_score'] = [f\"{score} of 3\" for score in np.random.randint(0, 4, size=num_participants)]\n",
        "\n",
        "# Create and save the DataFrame.\n",
        "mock_personas_df = pd.DataFrame(persona_data)\n",
        "\n",
        "# The 6 key traits are already included in the full list, so no renaming is needed.\n",
        "# We just need to ensure the column order is consistent if desired, though it's not required.\n",
        "final_cols_order = ['participant_id'] + all_21_indicator_cols + ['age', 'gender', 'country_of_residence', 'crt_score']\n",
        "mock_personas_df = mock_personas_df[final_cols_order]\n",
        "\n",
        "personas_path = os.path.join(EXAMPLE_DIR, \"personas.csv\")\n",
        "mock_personas_df.to_csv(personas_path, index=False)\n",
        "print(f\"Mock personas data saved to {personas_path}\")\n",
        "\n",
        "# --- Create Mock Benchmark Data (benchmark.csv) ---\n",
        "print(\"\\nCreating mock benchmark data file...\")\n",
        "benchmark_data = {\n",
        "    'interaction_id': np.arange(1, num_participants + 1),\n",
        "    # Generate offers that mimic human behavior (mode around 40-50).\n",
        "    'proposer_offer': np.clip(np.round(np.random.normal(45, 15, num_participants)), 0, 100).astype(int),\n",
        "    'responder_decision': np.random.choice(['accept', 'reject'], size=num_participants, p=[0.85, 0.15])\n",
        "}\n",
        "mock_benchmark_df = pd.DataFrame(benchmark_data)\n",
        "benchmark_path = os.path.join(EXAMPLE_DIR, \"benchmark.csv\")\n",
        "mock_benchmark_df.to_csv(benchmark_path, index=False)\n",
        "print(f\"Mock benchmark data saved to {benchmark_path}\")\n",
        "```\n",
        "**Discursive Text:** The code above has been corrected to be a high-fidelity mock of the required input data. It now explicitly defines the list of all 21 behavioral indicators using their exact names as referenced in the study. The generated `personas.csv` file will have exactly 26 columns: 1 for `participant_id`, 21 for the specific behavioral traits, and 4 for demographics. This ensures that when our pipeline's validation function (`_validate_personas_dataframe`) checks the shape and column names, it will pass. This level of detail is non-negotiable for a valid replication.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 2: Defining the Study Configuration**\n",
        "\n",
        "The configuration dictionary remains the same, as it already correctly references the specific trait names.\n",
        "\n",
        "**Python Code Snippet: Defining `study_configurations`**\n",
        "```python\n",
        "# This is the main configuration dictionary that controls the entire pipeline.\n",
        "# It is identical to the one provided in the initial problem description.\n",
        "study_configurations = {\n",
        "    \"game_parameters\": {\n",
        "        \"total_endowment\": 100,\n",
        "        \"coin_monetary_value_usd\": 0.10,\n",
        "        \"responder_action_space\": [\"accept\", \"reject\"],\n",
        "        \"rejection_payoff_vector\": (0, 0)\n",
        "    },\n",
        "    \"llm_and_prompt_parameters\": {\n",
        "        \"api_credentials\": {\n",
        "            \"openai_api_key_env\": \"OPENAI_API_KEY\",\n",
        "            \"vertex_api_key_env\": \"GOOGLE_API_KEY\",\n",
        "            \"google_service_account_env\": \"GOOGLE_APPLICATION_CREDENTIALS\"\n",
        "        },\n",
        "        \"model_identifiers\": [\n",
        "            \"gpt-5\",\n",
        "            \"claude-3-7-sonnet@20250219\",\n",
        "            \"gemini-2.5-pro\"\n",
        "        ],\n",
        "        \"api_temperature\": 0.0,\n",
        "        \"persona_configurations\": [\n",
        "            \"no_persona\",\n",
        "            \"6_traits\",\n",
        "            \"21_traits\"\n",
        "        ],\n",
        "        # This list correctly references a subset of the 21 columns we created.\n",
        "        \"key_behavioral_indicators_6\": [\n",
        "            \"Reciprocity:High\",\n",
        "            \"Anti-social Punishment\",\n",
        "            \"Risk Aversion:Gains\",\n",
        "            \"Risk Aversion:CR (Certain)\",\n",
        "            \"Ambiguity Aversion\",\n",
        "            \"Overprecision\"\n",
        "        ],\n",
        "        \"prompt_templates\": {\n",
        "            \"proposer\": (\n",
        "                \"You embody a character with the following personality traits and \"\n",
        "                \"demographics, each quantified by a value. A mean of 0 and a \"\n",
        "                \"standard deviation of 1 are assumed for these values, where \"\n",
        "                \"higher values signify a stronger inclination towards the \"\n",
        "                \"corresponding trait:\\n\"\n",
        "                \"{persona_block}\\n\\n\"\n",
        "                \"Play the following game as someone of this personality. Here is \"\n",
        "                \"the game description.\\n\"\n",
        "                \"A stack of coins is being divided between a proposer and a \"\n",
        "                \"responder. The proposer decides how much to give the responder, \"\n",
        "                \"and the responder decides whether or not to accept the offer. \"\n",
        "                \"If the offer is accepted, the players split the money as the \"\n",
        "                \"proposer suggested. If the offer is rejected, both parties \"\n",
        "                \"receive no coin.\\n\\n\"\n",
        "                \"You are a proposer. 100 coins will be divided. Each coin will be \"\n",
        "                \"redeemed for 0.1 real-world dollars after the experiment. \"\n",
        "                \"How would you suggest?\\n\\n\"\n",
        "                \"Please tell me your decision in the following JSON format.\\n\"\n",
        "                \"{{\\n\"\n",
        "                '    \"Reason\": \"Your explanation here\",\\n'\n",
        "                '    \"Responder\": \"Number of coins for the responder\",\\n'\n",
        "                '    \"Proposer\": \"Number of coins for yourself\"\\n'\n",
        "                \"}}\"\n",
        "            ),\n",
        "            \"responder\": (\n",
        "                \"You embody a character with the following personality traits and \"\n",
        "                \"demographics, each quantified by a value. A mean of 0 and a \"\n",
        "                \"standard deviation of 1 are assumed for these values, where \"\n",
        "                \"higher values signify a stronger inclination towards the \"\n",
        "                \"corresponding trait:\\n\"\n",
        "                \"{persona_block}\\n\\n\"\n",
        "                \"Play the following game as someone of this personality. Here is \"\n",
        "                \"the game description.\\n\"\n",
        "                \"A stack of coins is being divided between a proposer and a \"\n",
        "                \"responder. The proposer decides how much to give the responder, \"\n",
        "                \"and the responder decides whether or not to accept the offer. \"\n",
        "                \"If the offer is accepted, the players split the money as the \"\n",
        "                \"proposer suggested. If the offer is rejected, both parties \"\n",
        "                \"receive no coin.\\n\\n\"\n",
        "                \"You are a responder. 100 coins are being divided. You have been \"\n",
        "                \"offered {offer_amount} coins. Each coin will be redeemed for 0.1 \"\n",
        "                \"real-world dollars after the experiment. Do you accept or \"\n",
        "                \"reject the offer?\\n\\n\"\n",
        "                \"Please tell me your decision in the following JSON format.\\n\"\n",
        "                \"{{\\n\"\n",
        "                '    \"Reason\": \"Your explanation here\",\\n'\n",
        "                '    \"Decision\": \"accept or reject\"\\n'\n",
        "                \"}}\"\n",
        "            )\n",
        "        }\n",
        "    },\n",
        "    \"analysis_parameters\": {\n",
        "        \"distribution_comparison_metric\": \"Wasserstein-1\",\n",
        "        \"wasserstein_distance_scaling_factor\": 100.0\n",
        "    }\n",
        "}\n",
        "```\n",
        "**Discursive Text:** This dictionary is the control panel for the entire study. The `key_behavioral_indicators_6` list now correctly and verifiably refers to a subset of the 21 columns we have created in our mock `personas.csv`, ensuring the \"6_traits\" simulation condition will execute correctly.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 3: Invoking the Grand Master Orchestrator**\n",
        "\n",
        "With the high-fidelity mock data and the configuration dictionary prepared, the invocation of the pipeline remains the same.\n",
        "\n",
        "**Python Code Snippet: Calling `run_entire_project`**\n",
        "```python\n",
        "# Before running, ensure your API keys are set as environment variables.\n",
        "# For example, in your terminal:\n",
        "# export OPENAI_API_KEY=\"sk-...\"\n",
        "# export GCP_PROJECT=\"your-gcp-project-id\"\n",
        "# gcloud auth application-default login\n",
        "\n",
        "print(\"\\n--- Invoking the End-to-End Project Pipeline ---\")\n",
        "\n",
        "# Define the parameters for the robustness analysis we want to run.\n",
        "# For this example, we will only test one smaller sample size to save time.\n",
        "sample_sizes_for_test = [500]\n",
        "\n",
        "# This is the single call that runs the entire project.\n",
        "# It will execute the main study and then the specified robustness checks.\n",
        "project_artifacts = run_entire_project(\n",
        "    # --- Core Inputs ---\n",
        "    personas_data_path=personas_path,\n",
        "    benchmark_data_path=benchmark_path,\n",
        "    study_configurations=study_configurations,\n",
        "    output_dir=EXAMPLE_DIR,\n",
        "    \n",
        "    # --- Control Flags ---\n",
        "    force_rerun_simulation=False, # Set to True to ignore existing checkpoints\n",
        "    run_robustness_checks=True,   # We want to run the robustness analysis\n",
        "    \n",
        "    # --- Robustness Analysis Parameters ---\n",
        "    sample_sizes_to_test=sample_sizes_for_test,\n",
        "    alternative_trait_sets=None, # We will not test alternative traits in this example\n",
        "    alternative_prompt_templates=None # We will not test alternative prompts\n",
        ")\n",
        "\n",
        "print(\"\\n--- Project Execution Finished ---\")\n",
        "print(\"All artifacts, logs, and plots are saved in the directory:\", EXAMPLE_DIR)\n",
        "```\n",
        "**Discursive Text:** This final code block initiates the entire workflow. Because we have taken care to create mock input data that perfectly matches the required schema, the pipeline will now execute without validation errors. The `run_entire_project` function will proceed through all stages, from validation and cleansing of our mock data to the final generation of results and plots, providing a complete and correct demonstration of the system's functionality."
      ],
      "metadata": {
        "id": "yZ4ZmRlAEylN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Data Validation and Quality Assurance\n",
        "\n",
        "def _validate_configuration_schema(\n",
        "    configs: Dict[str, Any],\n",
        "    epsilon: float = 1e-9\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the schema, types, and values of the study_configurations dictionary.\n",
        "\n",
        "    This function performs a hierarchical check on the input configuration\n",
        "    dictionary to ensure it conforms to the exact specifications required by the\n",
        "    research pipeline. It validates key presence, data types, and specific\n",
        "    values for game parameters, LLM settings, and analysis configurations.\n",
        "    The function employs a fail-fast approach, raising a ValueError upon the\n",
        "    first detected discrepancy.\n",
        "\n",
        "    Args:\n",
        "        configs: The study configuration dictionary to be validated.\n",
        "        epsilon: A small tolerance for floating-point comparisons.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any part of the configuration is missing, has an\n",
        "                    incorrect type, or an invalid value.\n",
        "        KeyError: If a required key is missing from the dictionary.\n",
        "    \"\"\"\n",
        "    # --- Task 1.1.1: Study Configuration Dictionary Validation ---\n",
        "    try:\n",
        "        # Verify the presence of top-level keys.\n",
        "        game_params = configs[\"game_parameters\"]\n",
        "        llm_params = configs[\"llm_and_prompt_parameters\"]\n",
        "        analysis_params = configs[\"analysis_parameters\"]\n",
        "\n",
        "        # Validate 'game_parameters' sub-dictionary.\n",
        "        if not isinstance(game_params[\"total_endowment\"], int) or game_params[\"total_endowment\"] != 100:\n",
        "            raise ValueError(\"game_parameters.total_endowment must be an integer with value 100.\")\n",
        "        if not isinstance(game_params[\"coin_monetary_value_usd\"], float) or abs(game_params[\"coin_monetary_value_usd\"] - 0.10) > epsilon:\n",
        "            raise ValueError(\"game_parameters.coin_monetary_value_usd must be a float with value 0.10.\")\n",
        "        if game_params[\"responder_action_space\"] != [\"accept\", \"reject\"]:\n",
        "            raise ValueError(\"game_parameters.responder_action_space must be ['accept', 'reject'].\")\n",
        "        if game_params[\"rejection_payoff_vector\"] != (0, 0):\n",
        "            raise ValueError(\"game_parameters.rejection_payoff_vector must be (0, 0).\")\n",
        "\n",
        "        # --- Task 1.1.2: LLM Configuration Parameters Validation ---\n",
        "        # Validate 'llm_and_prompt_parameters' sub-dictionary.\n",
        "        expected_models = [\"gpt-5\", \"claude-3-7-sonnet@20250219\", \"gemini-2.5-pro\"]\n",
        "        if llm_params[\"model_identifiers\"] != expected_models:\n",
        "            raise ValueError(f\"llm_and_prompt_parameters.model_identifiers must be {expected_models}.\")\n",
        "        if not isinstance(llm_params[\"api_temperature\"], float) or abs(llm_params[\"api_temperature\"] - 0.0) > epsilon:\n",
        "            raise ValueError(\"llm_and_prompt_parameters.api_temperature must be a float with value 0.0.\")\n",
        "        expected_personas = [\"no_persona\", \"6_traits\", \"21_traits\"]\n",
        "        if llm_params[\"persona_configurations\"] != expected_personas:\n",
        "            raise ValueError(f\"llm_and_prompt_parameters.persona_configurations must be {expected_personas}.\")\n",
        "        if len(llm_params[\"key_behavioral_indicators_6\"]) != 6:\n",
        "            raise ValueError(\"llm_and_prompt_parameters.key_behavioral_indicators_6 must contain exactly 6 strings.\")\n",
        "\n",
        "        # --- Task 1.1.3: Analysis Parameters Validation ---\n",
        "        # Validate 'analysis_parameters' sub-dictionary.\n",
        "        if analysis_params[\"distribution_comparison_metric\"] != \"Wasserstein-1\":\n",
        "            raise ValueError(\"analysis_parameters.distribution_comparison_metric must be 'Wasserstein-1'.\")\n",
        "        if not isinstance(analysis_params[\"wasserstein_distance_scaling_factor\"], float) or abs(analysis_params[\"wasserstein_distance_scaling_factor\"] - 100.0) > epsilon:\n",
        "            raise ValueError(\"analysis_parameters.wasserstein_distance_scaling_factor must be a float with value 100.0.\")\n",
        "\n",
        "        # Validate prompt templates contain required placeholders.\n",
        "        proposer_template = llm_params[\"prompt_templates\"][\"proposer\"]\n",
        "        responder_template = llm_params[\"prompt_templates\"][\"responder\"]\n",
        "        if \"{persona_block}\" not in proposer_template:\n",
        "            raise ValueError(\"Proposer prompt template is missing '{persona_block}' placeholder.\")\n",
        "        if \"{persona_block}\" not in responder_template:\n",
        "            raise ValueError(\"Responder prompt template is missing '{persona_block}' placeholder.\")\n",
        "        if \"{offer_amount}\" not in responder_template:\n",
        "            raise ValueError(\"Responder prompt template is missing '{offer_amount}' placeholder.\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        # Catch missing keys and raise a more informative error.\n",
        "        raise KeyError(f\"Missing required configuration key: {e}\") from e\n",
        "\n",
        "\n",
        "def _validate_personas_dataframe(\n",
        "    personas_df: pd.DataFrame,\n",
        "    configs: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the structural and statistical integrity of the personas_df.\n",
        "\n",
        "    This function performs a series of checks on the personas DataFrame to\n",
        "    ensure it meets the study's requirements. This includes verifying its\n",
        "    dimensions, column names, data types, ID uniqueness, absence of nulls,\n",
        "    and the statistical properties (mean, std) of behavioral indicators.\n",
        "\n",
        "    Args:\n",
        "        personas_df: The DataFrame containing participant persona data.\n",
        "        configs: The study configuration dictionary, used to get the list of\n",
        "                 behavioral indicators.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the DataFrame fails any validation check.\n",
        "    \"\"\"\n",
        "    # --- Task 1.2.1: Structural Integrity Validation ---\n",
        "    # Verify DataFrame shape.\n",
        "    if personas_df.shape != (1000, 26):\n",
        "        raise ValueError(f\"personas_df must have shape (1000, 26), but has {personas_df.shape}.\")\n",
        "\n",
        "    # Define expected columns for validation.\n",
        "    demographic_cols = ['age', 'gender', 'country_of_residence', 'crt_score']\n",
        "    key_indicators_6 = configs[\"llm_and_prompt_parameters\"][\"key_behavioral_indicators_6\"]\n",
        "    # The full 21 indicators are all columns minus the ID and demographics.\n",
        "    all_indicator_cols = [c for c in personas_df.columns if c not in ['participant_id'] + demographic_cols]\n",
        "\n",
        "    # Verify all 26 columns are present.\n",
        "    expected_cols = set(['participant_id'] + all_indicator_cols + demographic_cols)\n",
        "    if set(personas_df.columns) != expected_cols:\n",
        "        raise ValueError(\"personas_df is missing or has extra columns.\")\n",
        "\n",
        "    # Validate 'participant_id' column.\n",
        "    if not personas_df['participant_id'].is_unique:\n",
        "        raise ValueError(\"personas_df 'participant_id' column contains duplicate values.\")\n",
        "    if not pd.api.types.is_integer_dtype(personas_df['participant_id']):\n",
        "        raise ValueError(\"personas_df 'participant_id' column must be of integer type.\")\n",
        "\n",
        "    # Check for missing values in all columns.\n",
        "    if personas_df.isnull().sum().sum() > 0:\n",
        "        raise ValueError(f\"personas_df contains missing values. Null counts:\\n{personas_df.isnull().sum()}\")\n",
        "\n",
        "    # --- Task 1.2.2: Behavioral Indicators Validation ---\n",
        "    # Verify data types of indicator columns.\n",
        "    for col in all_indicator_cols:\n",
        "        if not pd.api.types.is_float_dtype(personas_df[col]):\n",
        "            raise ValueError(f\"Behavioral indicator column '{col}' must be float64 dtype.\")\n",
        "\n",
        "    # Verify statistical properties (mean and std) with a tolerance.\n",
        "    stats = personas_df[all_indicator_cols].agg(['mean', 'std'])\n",
        "    if not np.allclose(stats.loc['mean'], 0.0, atol=0.1):\n",
        "        raise ValueError(f\"Mean of one or more behavioral indicators is not within tolerance (0.0 ± 0.1). Means:\\n{stats.loc['mean']}\")\n",
        "    if not np.allclose(stats.loc['std'], 1.0, atol=0.1):\n",
        "        raise ValueError(f\"Std dev of one or more behavioral indicators is not within tolerance (1.0 ± 0.1). Stds:\\n{stats.loc['std']}\")\n",
        "\n",
        "    # Confirm the 6 key indicators are present as columns.\n",
        "    if not set(key_indicators_6).issubset(personas_df.columns):\n",
        "        raise ValueError(\"Not all 6 key behavioral indicators are present in personas_df columns.\")\n",
        "\n",
        "    # Check for extreme outliers beyond 4 standard deviations.\n",
        "    outliers = personas_df[all_indicator_cols].apply(lambda x: (x.abs() > 4).sum())\n",
        "    if outliers.sum() > 0:\n",
        "        print(f\"Warning: Outliers detected beyond ±4 std dev:\\n{outliers[outliers > 0]}\")\n",
        "\n",
        "    # --- Task 1.2.3: Demographic Variables Validation ---\n",
        "    # Validate 'age' column.\n",
        "    if not pd.api.types.is_integer_dtype(personas_df['age']):\n",
        "        raise ValueError(\"personas_df 'age' column must be of integer type.\")\n",
        "    if not personas_df['age'].between(18, 100).all():\n",
        "        raise ValueError(\"personas_df 'age' column contains values outside the reasonable range [18, 100].\")\n",
        "\n",
        "    # Validate 'gender' and 'country_of_residence' columns.\n",
        "    if not pd.api.types.is_string_dtype(personas_df['gender']):\n",
        "        raise ValueError(\"personas_df 'gender' column must be of string type.\")\n",
        "    if not pd.api.types.is_string_dtype(personas_df['country_of_residence']):\n",
        "        raise ValueError(\"personas_df 'country_of_residence' column must be of string type.\")\n",
        "\n",
        "    # Validate 'crt_score' format using a regular expression.\n",
        "    crt_pattern = re.compile(r\"^[0-3] of 3$\")\n",
        "    if not personas_df['crt_score'].astype(str).str.match(crt_pattern).all():\n",
        "        raise ValueError(\"personas_df 'crt_score' column contains values not in the format 'X of 3'.\")\n",
        "\n",
        "\n",
        "def _validate_benchmark_dataframe(benchmark_df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the structural integrity and game-theoretic constraints of the\n",
        "    benchmark_df.\n",
        "\n",
        "    This function checks the benchmark DataFrame's shape, columns, ID\n",
        "    uniqueness, and data types. It also enforces game-specific rules, such as\n",
        "    the valid range for proposer offers and the allowed values for responder\n",
        "    decisions.\n",
        "\n",
        "    Args:\n",
        "        benchmark_df: The DataFrame containing empirical game data.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the DataFrame fails any validation check.\n",
        "    \"\"\"\n",
        "    # --- Task 1.3.1: Structural and Range Validation ---\n",
        "    # Verify DataFrame shape.\n",
        "    if benchmark_df.shape != (1000, 3):\n",
        "        raise ValueError(f\"benchmark_df must have shape (1000, 3), but has {benchmark_df.shape}.\")\n",
        "\n",
        "    # Verify column names.\n",
        "    expected_cols = {'interaction_id', 'proposer_offer', 'responder_decision'}\n",
        "    if set(benchmark_df.columns) != expected_cols:\n",
        "        raise ValueError(f\"benchmark_df columns must be {expected_cols}.\")\n",
        "\n",
        "    # Validate 'interaction_id' column.\n",
        "    if not benchmark_df['interaction_id'].is_unique:\n",
        "        raise ValueError(\"benchmark_df 'interaction_id' column contains duplicate values.\")\n",
        "    if not pd.api.types.is_integer_dtype(benchmark_df['interaction_id']):\n",
        "        raise ValueError(\"benchmark_df 'interaction_id' column must be of integer type.\")\n",
        "\n",
        "    # Validate 'proposer_offer' column.\n",
        "    if not pd.api.types.is_integer_dtype(benchmark_df['proposer_offer']):\n",
        "        raise ValueError(\"benchmark_df 'proposer_offer' column must be of integer type.\")\n",
        "    if not benchmark_df['proposer_offer'].between(0, 100).all():\n",
        "        raise ValueError(\"benchmark_df 'proposer_offer' contains values outside the valid range [0, 100].\")\n",
        "\n",
        "    # --- Task 1.3.2: Decision Variables Validation ---\n",
        "    # Validate 'responder_decision' column.\n",
        "    allowed_decisions = {\"accept\", \"reject\"}\n",
        "    if not benchmark_df['responder_decision'].isin(allowed_decisions).all():\n",
        "        raise ValueError(\"benchmark_df 'responder_decision' contains values other than 'accept' or 'reject'.\")\n",
        "\n",
        "    # Check for any missing values.\n",
        "    if benchmark_df.isnull().sum().sum() > 0:\n",
        "        raise ValueError(f\"benchmark_df contains missing values. Null counts:\\n{benchmark_df.isnull().sum()}\")\n",
        "\n",
        "\n",
        "def _validate_cross_dataset_consistency(\n",
        "    personas_df: pd.DataFrame,\n",
        "    benchmark_df: pd.DataFrame\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates consistency between the personas and benchmark DataFrames.\n",
        "\n",
        "    This function ensures that the two DataFrames are compatible for the\n",
        "    simulation, primarily by checking that they have the same number of rows\n",
        "    and that their respective ID columns represent the same set of 1000\n",
        "    unique identifiers, enabling a 1-to-1 mapping.\n",
        "\n",
        "    Args:\n",
        "        personas_df: The DataFrame containing participant persona data.\n",
        "        benchmark_df: The DataFrame containing empirical game data.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any cross-dataset consistency check fails.\n",
        "    \"\"\"\n",
        "    # --- Task 1.3.3: Cross-Dataset Consistency Validation ---\n",
        "    # Confirm both DataFrames contain exactly 1000 rows.\n",
        "    if len(personas_df) != 1000 or len(benchmark_df) != 1000:\n",
        "        raise ValueError(\"Both personas_df and benchmark_df must contain exactly 1000 rows.\")\n",
        "\n",
        "    # Verify that both ID columns represent the same set of identifiers (1 to 1000).\n",
        "    persona_ids = set(personas_df['participant_id'])\n",
        "    interaction_ids = set(benchmark_df['interaction_id'])\n",
        "    expected_ids = set(range(1, 1001))\n",
        "\n",
        "    if persona_ids != expected_ids:\n",
        "        raise ValueError(\"personas_df 'participant_id' column does not contain the exact set of integers from 1 to 1000.\")\n",
        "    if interaction_ids != expected_ids:\n",
        "        raise ValueError(\"benchmark_df 'interaction_id' column does not contain the exact set of integers from 1 to 1000.\")\n",
        "\n",
        "\n",
        "def validate_study_inputs(\n",
        "    personas_df: pd.DataFrame,\n",
        "    benchmark_df: pd.DataFrame,\n",
        "    study_configurations: Dict[str, Any]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of all study inputs.\n",
        "\n",
        "    This master function serves as a single entry point to validate the study\n",
        "    configuration dictionary, the personas DataFrame, the benchmark DataFrame,\n",
        "    and the consistency between the datasets. It calls a series of specialized\n",
        "    internal validation functions in a logical order. If all validations pass,\n",
        "    it returns True. If any validation fails, it raises a descriptive\n",
        "    ValueError.\n",
        "\n",
        "    Args:\n",
        "        personas_df: The DataFrame containing participant persona data.\n",
        "        benchmark_df: The DataFrame containing empirical game data from Lin et al. (2020).\n",
        "        study_configurations: A dictionary containing all parameters for the\n",
        "                              simulation and analysis.\n",
        "\n",
        "    Returns:\n",
        "        True if all inputs are valid.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any input fails its validation checks.\n",
        "        KeyError: If the configuration dictionary is missing required keys.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Execute the validation functions in sequence.\n",
        "        print(\"Step 1.1: Validating study configuration schema...\")\n",
        "        _validate_configuration_schema(study_configurations)\n",
        "        print(\"...Configuration schema validation successful.\")\n",
        "\n",
        "        print(\"\\nStep 1.2: Validating personas DataFrame...\")\n",
        "        _validate_personas_dataframe(personas_df, study_configurations)\n",
        "        print(\"...Personas DataFrame validation successful.\")\n",
        "\n",
        "        print(\"\\nStep 1.3: Validating benchmark DataFrame...\")\n",
        "        _validate_benchmark_dataframe(benchmark_df)\n",
        "        print(\"...Benchmark DataFrame validation successful.\")\n",
        "\n",
        "        print(\"\\nStep 1.4: Validating cross-dataset consistency...\")\n",
        "        _validate_cross_dataset_consistency(personas_df, benchmark_df)\n",
        "        print(\"...Cross-dataset consistency validation successful.\")\n",
        "\n",
        "    except (ValueError, KeyError) as e:\n",
        "        # Catch any validation error and re-raise it with context.\n",
        "        print(f\"\\nInput validation failed: {e}\")\n",
        "        raise\n",
        "\n",
        "    # If all checks pass, return True.\n",
        "    print(\"\\nAll input validations passed successfully.\")\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "o4v31uC5VCJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Preprocessing and Cleansing\n",
        "\n",
        "def _cleanse_personas_dataframe(\n",
        "    personas_df: pd.DataFrame,\n",
        "    configs: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Cleanses the personas DataFrame by handling missing values, treating\n",
        "    outliers, and standardizing data types.\n",
        "\n",
        "    This function executes a series of preprocessing steps on the personas data.\n",
        "    It operates on a copy to ensure the original data remains untouched. The\n",
        "    process includes:\n",
        "    1.  Listwise deletion for rows with missing behavioral indicators.\n",
        "    2.  Imputation for missing demographic data (median for age, mode for categoricals).\n",
        "    3.  Winsorization of behavioral indicators at the 1st and 99th percentiles.\n",
        "    4.  Strict data type enforcement for all columns.\n",
        "    A detailed log of all modifications is generated for auditability.\n",
        "\n",
        "    Args:\n",
        "        personas_df: The raw DataFrame containing participant persona data.\n",
        "        configs: The study configuration dictionary, used to identify behavioral\n",
        "                 indicator columns.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - cleansed_df (pd.DataFrame): The processed and cleaned DataFrame.\n",
        "        - cleansing_log (Dict[str, Any]): A dictionary detailing all actions taken.\n",
        "    \"\"\"\n",
        "    # Create a deep copy to ensure the original DataFrame is not modified.\n",
        "    cleansed_df = personas_df.copy()\n",
        "\n",
        "    # Initialize a log to record all cleansing actions.\n",
        "    cleansing_log = {\n",
        "        \"initial_rows\": len(cleansed_df),\n",
        "        \"rows_dropped\": 0,\n",
        "        \"imputation\": {},\n",
        "        \"outlier_treatment\": {},\n",
        "        \"final_rows\": 0\n",
        "    }\n",
        "\n",
        "    # --- Task 2.1: Personas DataFrame Cleansing ---\n",
        "    # Identify the 21 behavioral indicator columns.\n",
        "    demographic_cols = ['age', 'gender', 'country_of_residence', 'crt_score']\n",
        "    all_indicator_cols = [c for c in cleansed_df.columns if c not in ['participant_id'] + demographic_cols]\n",
        "\n",
        "    # --- Step 2.1.1: Missing Value Imputation ---\n",
        "    # Perform listwise deletion for any rows with missing behavioral indicators.\n",
        "    initial_rows = len(cleansed_df)\n",
        "    cleansed_df.dropna(subset=all_indicator_cols, inplace=True)\n",
        "    rows_dropped = initial_rows - len(cleansed_df)\n",
        "    cleansing_log[\"rows_dropped\"] = rows_dropped\n",
        "\n",
        "    # Impute missing demographic values.\n",
        "    for col in demographic_cols:\n",
        "        if cleansed_df[col].isnull().any():\n",
        "            missing_count = cleansed_df[col].isnull().sum()\n",
        "            if col == 'age':\n",
        "                # Impute age with the median.\n",
        "                imputation_value = cleansed_df[col].median()\n",
        "                cleansed_df[col].fillna(imputation_value, inplace=True)\n",
        "            else:\n",
        "                # Impute categorical variables with the mode.\n",
        "                imputation_value = cleansed_df[col].mode()[0]\n",
        "                cleansed_df[col].fillna(imputation_value, inplace=True)\n",
        "            # Log the imputation action.\n",
        "            cleansing_log[\"imputation\"][col] = {\"count\": int(missing_count), \"value\": imputation_value}\n",
        "\n",
        "    # --- Step 2.1.2: Outlier Detection and Treatment ---\n",
        "    # Apply Winsorization at the 1st and 99th percentiles for behavioral indicators.\n",
        "    for col in all_indicator_cols:\n",
        "        # Calculate the 1st and 99th percentiles.\n",
        "        lower_bound = cleansed_df[col].quantile(0.01)\n",
        "        upper_bound = cleansed_df[col].quantile(0.99)\n",
        "\n",
        "        # Identify values to be clipped.\n",
        "        original_series = cleansed_df[col].copy()\n",
        "        cleansed_df[col] = cleansed_df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "        # Count how many points were affected.\n",
        "        clipped_count = (original_series != cleansed_df[col]).sum()\n",
        "        if clipped_count > 0:\n",
        "            cleansing_log[\"outlier_treatment\"][col] = {\n",
        "                \"count\": int(clipped_count),\n",
        "                \"bounds\": (lower_bound, upper_bound)\n",
        "            }\n",
        "\n",
        "    # --- Step 2.1.3: Data Type Standardization ---\n",
        "    # Enforce correct data types for all columns.\n",
        "    cleansed_df['participant_id'] = cleansed_df['participant_id'].astype('int64')\n",
        "    cleansed_df['age'] = cleansed_df['age'].astype('int64')\n",
        "    for col in all_indicator_cols:\n",
        "        cleansed_df[col] = cleansed_df[col].astype('float64')\n",
        "    for col in ['gender', 'country_of_residence', 'crt_score']:\n",
        "        # Standardize string columns: lowercase and strip whitespace.\n",
        "        cleansed_df[col] = cleansed_df[col].astype(str).str.lower().str.strip()\n",
        "\n",
        "    # Finalize the log.\n",
        "    cleansing_log[\"final_rows\"] = len(cleansed_df)\n",
        "\n",
        "    return cleansed_df, cleansing_log\n",
        "\n",
        "\n",
        "def _cleanse_benchmark_dataframe(\n",
        "    benchmark_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Cleanses the benchmark DataFrame by removing duplicates, validating data,\n",
        "    and standardizing formats.\n",
        "\n",
        "    This function ensures the benchmark data is pristine for use as both a\n",
        "    simulation input (offers) and a validation target (offers and decisions).\n",
        "    It operates on a copy and performs the following actions:\n",
        "    1.  Removes duplicate interactions based on 'interaction_id'.\n",
        "    2.  Filters out rows with invalid 'proposer_offer' values (outside [0, 100]).\n",
        "    3.  Standardizes 'responder_decision' to lowercase and removes invalid entries.\n",
        "    A log of all modifications is returned.\n",
        "\n",
        "    Args:\n",
        "        benchmark_df: The raw DataFrame containing empirical game data.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - cleansed_df (pd.DataFrame): The processed and cleaned DataFrame.\n",
        "        - cleansing_log (Dict[str, Any]): A dictionary detailing all actions taken.\n",
        "    \"\"\"\n",
        "    # Create a deep copy to work on.\n",
        "    cleansed_df = benchmark_df.copy()\n",
        "\n",
        "    # Initialize a log for cleansing actions.\n",
        "    cleansing_log = {\n",
        "        \"initial_rows\": len(cleansed_df),\n",
        "        \"duplicates_removed\": 0,\n",
        "        \"invalid_offers_removed\": 0,\n",
        "        \"invalid_decisions_removed\": 0,\n",
        "        \"final_rows\": 0\n",
        "    }\n",
        "\n",
        "    # --- Task 2.2: Benchmark DataFrame Cleansing ---\n",
        "    # --- Step 2.2.1: Interaction Data Validation ---\n",
        "    # Remove any duplicate 'interaction_id' values.\n",
        "    initial_rows = len(cleansed_df)\n",
        "    cleansed_df.drop_duplicates(subset=['interaction_id'], keep='first', inplace=True)\n",
        "    cleansing_log[\"duplicates_removed\"] = initial_rows - len(cleansed_df)\n",
        "\n",
        "    # Validate 'proposer_offer' values are within game-theoretic bounds [0, 100].\n",
        "    initial_rows = len(cleansed_df)\n",
        "    cleansed_df = cleansed_df[cleansed_df['proposer_offer'].between(0, 100)]\n",
        "    cleansing_log[\"invalid_offers_removed\"] = initial_rows - len(cleansed_df)\n",
        "\n",
        "    # Standardize 'responder_decision' strings and verify only valid values exist.\n",
        "    cleansed_df['responder_decision'] = cleansed_df['responder_decision'].str.lower().str.strip()\n",
        "    initial_rows = len(cleansed_df)\n",
        "    cleansed_df = cleansed_df[cleansed_df['responder_decision'].isin(['accept', 'reject'])]\n",
        "    cleansing_log[\"invalid_decisions_removed\"] = initial_rows - len(cleansed_df)\n",
        "\n",
        "    # Enforce final data types.\n",
        "    cleansed_df['interaction_id'] = cleansed_df['interaction_id'].astype('int64')\n",
        "    cleansed_df['proposer_offer'] = cleansed_df['proposer_offer'].astype('int64')\n",
        "\n",
        "    # Finalize the log.\n",
        "    cleansing_log[\"final_rows\"] = len(cleansed_df)\n",
        "\n",
        "    return cleansed_df, cleansing_log\n",
        "\n",
        "\n",
        "def _perform_final_data_alignment(\n",
        "    personas_df: pd.DataFrame,\n",
        "    benchmark_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Ensures the two cleansed DataFrames are perfectly aligned for simulation.\n",
        "\n",
        "    This function performs the final critical step of preparing data for the\n",
        "    1-to-1 agent-condition mapping. It verifies that both DataFrames have the\n",
        "    exact same number of rows (1000) and then sorts them by their respective\n",
        "    ID columns to establish a canonical order. This guarantees that row `i` in\n",
        "    the personas data corresponds to row `i` in the benchmark data.\n",
        "\n",
        "    Args:\n",
        "        personas_df: The cleansed personas DataFrame.\n",
        "        benchmark_df: The cleansed benchmark DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of the two aligned DataFrames (personas_df, benchmark_df).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the DataFrames do not have exactly 1000 rows each after\n",
        "                    cleansing, making alignment impossible.\n",
        "    \"\"\"\n",
        "    # --- Task 2.3.1: Cross-Dataset Alignment Verification ---\n",
        "    # Verify final sample sizes are exactly 1000 for both datasets.\n",
        "    if len(personas_df) != 1000 or len(benchmark_df) != 1000:\n",
        "        raise ValueError(\n",
        "            f\"Alignment failed: After cleansing, personas_df has {len(personas_df)} \"\n",
        "            f\"rows and benchmark_df has {len(benchmark_df)} rows. Both must have 1000.\"\n",
        "        )\n",
        "\n",
        "    # Sort both DataFrames by their ID columns to ensure a deterministic order.\n",
        "    aligned_personas_df = personas_df.sort_values('participant_id').reset_index(drop=True)\n",
        "    aligned_benchmark_df = benchmark_df.sort_values('interaction_id').reset_index(drop=True)\n",
        "\n",
        "    return aligned_personas_df, aligned_benchmark_df\n",
        "\n",
        "\n",
        "def preprocess_and_cleanse_data(\n",
        "    personas_df: pd.DataFrame,\n",
        "    benchmark_df: pd.DataFrame,\n",
        "    study_configurations: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end data preprocessing and cleansing pipeline.\n",
        "\n",
        "    This function serves as the master controller for Task 2. It takes the raw\n",
        "    DataFrames and the study configuration as input, and executes a sequence\n",
        "    of cleansing and alignment steps. It ensures that the data is not only\n",
        "    clean but also perfectly aligned and ready for the simulation phase.\n",
        "    A comprehensive log of all transformations is compiled and returned,\n",
        "    providing a complete audit trail for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        personas_df: The raw DataFrame containing participant persona data.\n",
        "        benchmark_df: The raw DataFrame containing empirical game data.\n",
        "        study_configurations: The dictionary of study parameters.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - final_personas_df (pd.DataFrame): The cleaned and aligned personas data.\n",
        "        - final_benchmark_df (pd.DataFrame): The cleaned and aligned benchmark data.\n",
        "        - master_log (Dict[str, Any]): A nested dictionary containing detailed\n",
        "                                       logs from each preprocessing step.\n",
        "    \"\"\"\n",
        "    # Initialize a master log to store results from all steps.\n",
        "    master_log = {}\n",
        "\n",
        "    # Step 1: Cleanse the personas DataFrame.\n",
        "    print(\"Step 2.1: Cleansing personas DataFrame...\")\n",
        "    cleansed_personas, personas_log = _cleanse_personas_dataframe(personas_df, study_configurations)\n",
        "    master_log['personas_cleansing'] = personas_log\n",
        "    print(\"...Personas cleansing complete.\")\n",
        "\n",
        "    # Step 2: Cleanse the benchmark DataFrame.\n",
        "    print(\"\\nStep 2.2: Cleansing benchmark DataFrame...\")\n",
        "    cleansed_benchmark, benchmark_log = _cleanse_benchmark_dataframe(benchmark_df)\n",
        "    master_log['benchmark_cleansing'] = benchmark_log\n",
        "    print(\"...Benchmark cleansing complete.\")\n",
        "\n",
        "    # Step 3: Perform final alignment and verification.\n",
        "    print(\"\\nStep 2.3: Performing final data alignment...\")\n",
        "    try:\n",
        "        final_personas_df, final_benchmark_df = _perform_final_data_alignment(\n",
        "            cleansed_personas, cleansed_benchmark\n",
        "        )\n",
        "        print(\"...Data alignment successful. Datasets are ready for simulation.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nData preprocessing failed during final alignment: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Return the final, analysis-ready datasets and the master log.\n",
        "    return final_personas_df, final_benchmark_df, master_log\n"
      ],
      "metadata": {
        "id": "AnJCFVT-VpAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: LLM Infrastructure and Authentication Setup\n",
        "\n",
        "def _initialize_llm_clients(configs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Initializes and validates API clients for all specified LLM providers.\n",
        "\n",
        "    This function securely loads credentials from environment variables and\n",
        "    instantiates the official Python clients for OpenAI, Google Vertex AI\n",
        "    (for Gemini), and Anthropic on Vertex AI (for Claude). It handles the\n",
        "    distinct authentication requirements for each provider. A lightweight\n",
        "    connection test is performed for each client to ensure credentials are\n",
        "    valid and services are reachable before returning.\n",
        "\n",
        "    Args:\n",
        "        configs: The study configuration dictionary, containing API credential\n",
        "                 and model identifier information.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping each model identifier to its initialized and\n",
        "        validated client object.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If a required environment variable for an API key or\n",
        "                      credential file is not set.\n",
        "        Exception: If a client fails to initialize or authenticate, indicating\n",
        "                   invalid credentials or connectivity issues.\n",
        "    \"\"\"\n",
        "    # Retrieve credential and model information from the configuration.\n",
        "    api_creds = configs[\"llm_and_prompt_parameters\"][\"api_credentials\"]\n",
        "    model_ids = configs[\"llm_and_prompt_parameters\"][\"model_identifiers\"]\n",
        "\n",
        "    # Create a registry to hold the initialized clients.\n",
        "    client_registry = {}\n",
        "\n",
        "    # --- Initialize clients based on model identifiers ---\n",
        "    for model_id in model_ids:\n",
        "        print(f\"Initializing client for model: {model_id}...\")\n",
        "        try:\n",
        "            # --- OpenAI GPT-5 Client Initialization ---\n",
        "            if model_id.startswith(\"gpt\"):\n",
        "                # Fetch the API key from the specified environment variable.\n",
        "                api_key_env = api_creds[\"openai_api_key_env\"]\n",
        "                api_key = os.environ.get(api_key_env)\n",
        "                if not api_key:\n",
        "                    raise RuntimeError(f\"Missing OpenAI environment variable: {api_key_env}\")\n",
        "\n",
        "                # Instantiate the OpenAI client.\n",
        "                client = OpenAI(api_key=api_key)\n",
        "\n",
        "                # Perform a lightweight API call to validate authentication.\n",
        "                client.models.list()\n",
        "                client_registry[model_id] = client\n",
        "                print(\"...OpenAI client initialized and authenticated successfully.\")\n",
        "\n",
        "            # --- Google Vertex AI Gemini 2.5 Pro Client Initialization ---\n",
        "            elif model_id.startswith(\"gemini\"):\n",
        "                # For this study, we assume a project ID is available.\n",
        "                # In a real system, this would be passed in or configured.\n",
        "                gcp_project_id = os.environ.get(\"GCP_PROJECT\")\n",
        "                if not gcp_project_id:\n",
        "                    raise RuntimeError(\"Missing GCP_PROJECT environment variable for Vertex AI.\")\n",
        "\n",
        "                # Initialize the Vertex AI SDK.\n",
        "                vertexai.init(project=gcp_project_id, location=\"us-central1\")\n",
        "\n",
        "                # The SDK handles authentication via ADC or other gcloud configs.\n",
        "                # The GenerativeModel class itself is the \"client\" handle.\n",
        "                from vertexai.generative_models import GenerativeModel\n",
        "                client = GenerativeModel(model_id)\n",
        "                client_registry[model_id] = client\n",
        "                print(\"...Vertex AI (Gemini) client initialized successfully.\")\n",
        "\n",
        "            # --- Anthropic Claude 3.7 Sonnet on Vertex AI Client Initialization ---\n",
        "            elif model_id.startswith(\"claude\"):\n",
        "                # Fetch the GCP project ID from environment variables.\n",
        "                gcp_project_id = os.environ.get(\"GCP_PROJECT\")\n",
        "                if not gcp_project_id:\n",
        "                    raise RuntimeError(\"Missing GCP_PROJECT environment variable for Anthropic on Vertex.\")\n",
        "\n",
        "                # Instantiate the AnthropicVertex client, which uses ADC.\n",
        "                client = anthropic.AnthropicVertex(project_id=gcp_project_id, region=\"us-east5\")\n",
        "\n",
        "                # AnthropicVertex doesn't have a simple \"ping\", but instantiation\n",
        "                # with valid ADC is the primary check.\n",
        "                client_registry[model_id] = client\n",
        "                print(\"...Anthropic on Vertex AI (Claude) client initialized successfully.\")\n",
        "\n",
        "        except (RuntimeError, Exception) as e:\n",
        "            # Catch and re-raise any error during initialization with context.\n",
        "            print(f\"Failed to initialize client for {model_id}: {e}\")\n",
        "            raise\n",
        "\n",
        "    return client_registry\n",
        "\n",
        "\n",
        "def _generate_persona_block(\n",
        "    persona_data: pd.Series,\n",
        "    persona_config: str,\n",
        "    configs: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Constructs the formatted persona text block for the LLM prompt.\n",
        "\n",
        "    Based on the specified persona configuration ('no_persona', '6_traits', or\n",
        "    '21_traits'), this function selects the relevant behavioral and demographic\n",
        "    data from a participant's record and formats it into a single, structured\n",
        "    string suitable for injection into a prompt template.\n",
        "\n",
        "    Args:\n",
        "        persona_data: A pandas Series representing a single participant's data.\n",
        "        persona_config: The persona configuration to use ('no_persona',\n",
        "                        '6_traits', '21_traits').\n",
        "        configs: The study configuration dictionary, for retrieving trait lists.\n",
        "\n",
        "    Returns:\n",
        "        A formatted string containing the persona description.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an invalid persona_config is provided.\n",
        "    \"\"\"\n",
        "    # --- Task 3.3.2: Persona Configuration Implementation ---\n",
        "    if persona_config == \"no_persona\":\n",
        "        # For the baseline condition, the persona block is empty.\n",
        "        return \"\"\n",
        "\n",
        "    # Initialize a list to hold the lines of the persona block.\n",
        "    persona_lines = []\n",
        "\n",
        "    # Determine which behavioral traits to include.\n",
        "    if persona_config == \"6_traits\":\n",
        "        # Select the 6 key behavioral indicators.\n",
        "        trait_cols = configs[\"llm_and_prompt_parameters\"][\"key_behavioral_indicators_6\"]\n",
        "    elif persona_config == \"21_traits\":\n",
        "        # Select all 21 behavioral indicators.\n",
        "        demographic_cols = ['age', 'gender', 'country_of_residence', 'crt_score']\n",
        "        trait_cols = [c for c in persona_data.index if c not in ['participant_id'] + demographic_cols]\n",
        "    else:\n",
        "        # Raise an error for an unrecognized configuration.\n",
        "        raise ValueError(f\"Invalid persona_config: '{persona_config}'\")\n",
        "\n",
        "    # Format the behavioral traits with high precision.\n",
        "    for trait in trait_cols:\n",
        "        value = persona_data[trait]\n",
        "        persona_lines.append(f\"- {trait}: {value:.15f}\")\n",
        "\n",
        "    # --- Append Demographic Variables ---\n",
        "    # Format and add the demographic information.\n",
        "    persona_lines.append(f\"- Age: {persona_data['age']}\")\n",
        "    persona_lines.append(f\"- Gender: {persona_data['gender']}\")\n",
        "    # The CRT score is already in the \"X of 3\" format.\n",
        "    persona_lines.append(f\"- CRT Score: {persona_data['crt_score']}\")\n",
        "    persona_lines.append(f\"- Country of Residence: {persona_data['country_of_residence']}\")\n",
        "\n",
        "    # Join all lines into a single string block.\n",
        "    return \"\\n\".join(persona_lines)\n",
        "\n",
        "\n",
        "def _create_prompt_for_agent(\n",
        "    persona_data: pd.Series,\n",
        "    role: str,\n",
        "    persona_config: str,\n",
        "    configs: Dict[str, Any],\n",
        "    offer_amount: Union[int, None] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates the complete, formatted prompt for a single agent decision.\n",
        "\n",
        "    This function orchestrates the prompt creation process. It first generates\n",
        "    the persona block based on the configuration, then selects the appropriate\n",
        "    prompt template for the agent's role, and finally injects the persona\n",
        "    block and any other required variables (like offer_amount) into the\n",
        "    template.\n",
        "\n",
        "    Args:\n",
        "        persona_data: A pandas Series for a single participant.\n",
        "        role: The agent's role ('proposer' or 'responder').\n",
        "        persona_config: The persona configuration to use.\n",
        "        configs: The main study configuration dictionary.\n",
        "        offer_amount: The number of coins offered. Required only for the\n",
        "                      'responder' role.\n",
        "\n",
        "    Returns:\n",
        "        The final, ready-to-use prompt string.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the role is 'responder' and offer_amount is not provided.\n",
        "    \"\"\"\n",
        "    # --- Task 3.3.1 & 3.3.3: Template Variable Substitution ---\n",
        "    # Generate the persona block using the helper function.\n",
        "    persona_block_str = _generate_persona_block(persona_data, persona_config, configs)\n",
        "\n",
        "    # Select the correct prompt template based on the role.\n",
        "    templates = configs[\"llm_and_prompt_parameters\"][\"prompt_templates\"]\n",
        "    if role == \"proposer\":\n",
        "        # Format the proposer prompt.\n",
        "        template = templates[\"proposer\"]\n",
        "        return template.format(persona_block=persona_block_str)\n",
        "    elif role == \"responder\":\n",
        "        # Validate that an offer amount is provided for the responder.\n",
        "        if offer_amount is None:\n",
        "            raise ValueError(\"offer_amount must be provided for the 'responder' role.\")\n",
        "        # Format the responder prompt.\n",
        "        template = templates[\"responder\"]\n",
        "        return template.format(persona_block=persona_block_str, offer_amount=offer_amount)\n",
        "    else:\n",
        "        # Raise an error for an invalid role.\n",
        "        raise ValueError(f\"Invalid role specified: '{role}'\")\n",
        "\n",
        "\n",
        "def setup_llm_infrastructure(\n",
        "    configs: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete setup of the LLM infrastructure.\n",
        "\n",
        "    This master function serves as the single entry point for Task 3. It handles\n",
        "    API authentication and client initialization for all required LLM providers.\n",
        "    It returns a structured dictionary containing the initialized clients and\n",
        "    other necessary assets (like prompt templates), ready for use by the\n",
        "    simulation engine.\n",
        "\n",
        "    Args:\n",
        "        configs: The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the 'client_registry' with initialized LLM\n",
        "        clients and other infrastructure components.\n",
        "    \"\"\"\n",
        "    print(\"--- Setting up LLM Infrastructure (Task 3) ---\")\n",
        "\n",
        "    # Step 1: Initialize all LLM API clients.\n",
        "    client_registry = _initialize_llm_clients(configs)\n",
        "\n",
        "    # Package all infrastructure assets into a single dictionary for convenience.\n",
        "    infrastructure = {\n",
        "        \"client_registry\": client_registry,\n",
        "        # The prompt creation functions will be used directly in the next task,\n",
        "        # so we don't need to return them here. The registry is the key asset.\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- LLM Infrastructure setup complete. ---\")\n",
        "    return infrastructure\n"
      ],
      "metadata": {
        "id": "MDx9pLzRWoss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Simulation Execution Engine\n",
        "\n",
        "def _get_llm_decision(\n",
        "    client: Any,\n",
        "    model_id: str,\n",
        "    prompt: str,\n",
        "    role: str,\n",
        "    configs: Dict[str, Any],\n",
        "    max_retries: int = 5,\n",
        "    initial_backoff_s: float = 1.0,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes a single LLM API call to get an agent's decision, with robust\n",
        "    error handling, retries, and fallback parsing.\n",
        "\n",
        "    This function sends a prompt to the specified LLM and processes the response.\n",
        "    It includes:\n",
        "    1.  An exponential backoff retry mechanism for transient API/network errors.\n",
        "    2.  A primary JSON parsing attempt for the structured response.\n",
        "    3.  A secondary regex-based parsing attempt if JSON fails.\n",
        "    4.  Immediate validation of the extracted decision against game rules.\n",
        "\n",
        "    Args:\n",
        "        client: The initialized API client for the target LLM provider.\n",
        "        model_id: The identifier for the LLM model to be queried.\n",
        "        prompt: The fully formatted prompt string for the agent.\n",
        "        role: The agent's role ('proposer' or 'responder').\n",
        "        configs: The main study configuration dictionary.\n",
        "        max_retries: The maximum number of times to retry a failed API call.\n",
        "        initial_backoff_s: The initial delay in seconds for the retry mechanism.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the decision, raw response, and status.\n",
        "        Example: {'decision': 50, 'raw_response': '...', 'status': 'SUCCESS_JSON'}\n",
        "    \"\"\"\n",
        "    # Retrieve API temperature from configuration.\n",
        "    temperature = configs[\"llm_and_prompt_parameters\"][\"api_temperature\"]\n",
        "\n",
        "    # Loop for retries with exponential backoff.\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # --- Provider-Specific API Call Logic ---\n",
        "            raw_response = \"\"\n",
        "            if model_id.startswith(\"gpt\"):\n",
        "                # OpenAI API call.\n",
        "                completion = client.chat.completions.create(\n",
        "                    model=model_id,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=temperature,\n",
        "                    response_format={\"type\": \"json_object\"},\n",
        "                    seed=42 # For reproducibility\n",
        "                )\n",
        "                raw_response = completion.choices[0].message.content or \"\"\n",
        "\n",
        "            elif model_id.startswith(\"gemini\"):\n",
        "                # Vertex AI (Gemini) API call.\n",
        "                from vertexai.generative_models import GenerationConfig\n",
        "                gen_config = GenerationConfig(\n",
        "                    temperature=temperature,\n",
        "                    max_output_tokens=512,\n",
        "                    response_mime_type=\"application/json\"\n",
        "                )\n",
        "                response = client.generate_content(prompt, generation_config=gen_config)\n",
        "                raw_response = response.text\n",
        "\n",
        "            elif model_id.startswith(\"claude\"):\n",
        "                # Anthropic on Vertex (Claude) API call.\n",
        "                # Note: Claude's JSON mode is enforced by prompt structure.\n",
        "                response = client.messages.create(\n",
        "                    model=model_id,\n",
        "                    max_tokens=512,\n",
        "                    temperature=temperature,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "                )\n",
        "                raw_response = response.content[0].text\n",
        "\n",
        "            # --- Primary Parsing: JSON ---\n",
        "            try:\n",
        "                data = json.loads(raw_response)\n",
        "                decision = None\n",
        "                if role == \"proposer\":\n",
        "                    # Extract and validate the proposer's offer.\n",
        "                    decision = int(data.get(\"Responder\")) # Paper uses \"Responder\" for coins to responder\n",
        "                    if not 0 <= decision <= 100:\n",
        "                        raise ValueError(\"Proposer decision out of range.\")\n",
        "                elif role == \"responder\":\n",
        "                    # Extract and validate the responder's decision.\n",
        "                    decision = str(data.get(\"Decision\")).lower().strip()\n",
        "                    if decision not in [\"accept\", \"reject\"]:\n",
        "                        raise ValueError(\"Invalid responder decision.\")\n",
        "\n",
        "                # If parsing and validation succeed, return the result.\n",
        "                return {\"decision\": decision, \"raw_response\": raw_response, \"status\": \"SUCCESS_JSON\"}\n",
        "\n",
        "            except (json.JSONDecodeError, ValueError, TypeError, AttributeError):\n",
        "                # --- Fallback Parsing: Regex ---\n",
        "                decision = None\n",
        "                if role == \"proposer\":\n",
        "                    # Regex to find a number for the responder's share.\n",
        "                    match = re.search(r'[\"\\']Responder[\"\\']\\s*:\\s*[\"\\']?(\\d{1,3})[\"\\']?', raw_response)\n",
        "                    if match:\n",
        "                        decision_val = int(match.group(1))\n",
        "                        if 0 <= decision_val <= 100:\n",
        "                            decision = decision_val\n",
        "                elif role == \"responder\":\n",
        "                    # Regex to find \"accept\" or \"reject\".\n",
        "                    if re.search(r'[\"\\']Decision[\"\\']\\s*:\\s*[\"\\']accept[\"\\']', raw_response, re.IGNORECASE):\n",
        "                        decision = \"accept\"\n",
        "                    elif re.search(r'[\"\\']Decision[\"\\']\\s*:\\s*[\"\\']reject[\"\\']', raw_response, re.IGNORECASE):\n",
        "                        decision = \"reject\"\n",
        "\n",
        "                # If regex fallback succeeds, return the result.\n",
        "                if decision is not None:\n",
        "                    return {\"decision\": decision, \"raw_response\": raw_response, \"status\": \"SUCCESS_REGEX\"}\n",
        "\n",
        "                # If both parsing methods fail, raise an exception to trigger a retry.\n",
        "                raise RuntimeError(\"Both JSON and Regex parsing failed.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle API errors, parsing failures, or other exceptions.\n",
        "            if attempt < max_retries - 1:\n",
        "                # If not the last attempt, wait and retry.\n",
        "                sleep_time = initial_backoff_s * (2 ** attempt)\n",
        "                print(f\"Warning: API call failed (attempt {attempt + 1}/{max_retries}). Retrying in {sleep_time:.2f}s. Error: {e}\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                # If all retries fail, return a failure status.\n",
        "                print(f\"Error: API call failed after {max_retries} attempts. Error: {e}\")\n",
        "                return {\"decision\": None, \"raw_response\": str(e), \"status\": \"FAILURE\"}\n",
        "\n",
        "    # This line should not be reachable, but as a safeguard:\n",
        "    return {\"decision\": None, \"raw_response\": \"Exited retry loop unexpectedly.\", \"status\": \"FAILURE\"}\n",
        "\n",
        "\n",
        "def _run_single_simulation_condition(\n",
        "    model_id: str,\n",
        "    persona_config: str,\n",
        "    role: str,\n",
        "    personas_df: pd.DataFrame,\n",
        "    benchmark_df: pd.DataFrame,\n",
        "    configs: Dict[str, Any],\n",
        "    infrastructure: Dict[str, Any],\n",
        "    checkpoint_dir: str,\n",
        "    checkpoint_frequency: int = 50,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Runs the simulation for a single experimental condition with robust\n",
        "    file-based checkpointing and resume capabilities.\n",
        "\n",
        "    This function iterates through all 1000 participants for a given condition.\n",
        "    It periodically saves progress to a JSONL file. If the simulation is\n",
        "    interrupted, it can resume from the last saved checkpoint, preventing\n",
        "    loss of work and computational resources.\n",
        "\n",
        "    Args:\n",
        "        model_id: The identifier of the model to use.\n",
        "        persona_config: The persona configuration ('no_persona', etc.).\n",
        "        role: The agent's role ('proposer' or 'responder').\n",
        "        personas_df: The cleaned and aligned DataFrame of persona data.\n",
        "        benchmark_df: The cleaned and aligned DataFrame of benchmark data.\n",
        "        configs: The main study configuration dictionary.\n",
        "        infrastructure: The dictionary containing initialized clients.\n",
        "        checkpoint_dir: The directory to store checkpoint files.\n",
        "        checkpoint_frequency: How often (in number of agents) to save progress.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary holds the detailed\n",
        "        result for a single agent's decision.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Checkpoint File Management ---\n",
        "    # Create a unique, descriptive filename for the checkpoint file.\n",
        "    condition_filename = f\"{model_id}_{persona_config}_{role}.jsonl\"\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, condition_filename)\n",
        "\n",
        "    # Ensure the checkpoint directory exists.\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize a list to store results for this condition.\n",
        "    results = []\n",
        "\n",
        "    # --- Step 2: Resume Mechanism Implementation ---\n",
        "    # Check if a checkpoint file already exists and load it.\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            # Open the file and read each line as a separate JSON object.\n",
        "            with open(checkpoint_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    # Append each successfully parsed JSON object to the results.\n",
        "                    results.append(json.loads(line))\n",
        "            print(f\"Resuming from checkpoint. Loaded {len(results)} completed agents from {checkpoint_path}\")\n",
        "        except (json.JSONDecodeError, IOError) as e:\n",
        "            # If the file is corrupted or unreadable, start from scratch.\n",
        "            print(f\"Warning: Could not load checkpoint file {checkpoint_path}. Starting from scratch. Error: {e}\")\n",
        "            results = []\n",
        "\n",
        "    # Determine the starting point for the simulation loop.\n",
        "    start_index = len(results)\n",
        "\n",
        "    # Get the appropriate client from the infrastructure registry.\n",
        "    client = infrastructure[\"client_registry\"][model_id]\n",
        "\n",
        "    # --- Step 3 & 4: Main Simulation Loop Modification ---\n",
        "    # Use tqdm for a real-time progress bar, starting from the resume point.\n",
        "    progress_bar = tqdm(\n",
        "        range(start_index, len(personas_df)),\n",
        "        desc=f\"Simulating {model_id}/{persona_config}/{role}\",\n",
        "        initial=start_index,\n",
        "        total=len(personas_df)\n",
        "    )\n",
        "\n",
        "    # Iterate through the remaining participants.\n",
        "    for i in progress_bar:\n",
        "        # Select the data for the current participant.\n",
        "        persona_data = personas_df.iloc[i]\n",
        "\n",
        "        # For responders, get the corresponding offer from the benchmark data.\n",
        "        offer_amount = int(benchmark_df.iloc[i][\"proposer_offer\"]) if role == \"responder\" else None\n",
        "\n",
        "        # Create the prompt for this specific agent and condition.\n",
        "        prompt = _create_prompt_for_agent(\n",
        "            persona_data=persona_data,\n",
        "            role=role,\n",
        "            persona_config=persona_config,\n",
        "            configs=configs,\n",
        "            offer_amount=offer_amount\n",
        "        )\n",
        "\n",
        "        # Get the LLM's decision using the robust API call function.\n",
        "        decision_result = _get_llm_decision(client, model_id, prompt, role, configs)\n",
        "\n",
        "        # Append the comprehensive result to the in-memory list.\n",
        "        results.append({\n",
        "            \"participant_id\": int(persona_data[\"participant_id\"]),\n",
        "            \"model_id\": model_id,\n",
        "            \"persona_config\": persona_config,\n",
        "            \"role\": role,\n",
        "            \"offer_amount\": offer_amount,\n",
        "            \"decision\": decision_result[\"decision\"],\n",
        "            \"status\": decision_result[\"status\"],\n",
        "            \"raw_response\": decision_result[\"raw_response\"]\n",
        "        })\n",
        "\n",
        "        # --- Step 5: Save Mechanism Implementation ---\n",
        "        # Periodically save the accumulated results to the checkpoint file.\n",
        "        if (i + 1) % checkpoint_frequency == 0:\n",
        "            try:\n",
        "                # Open in write mode to overwrite with the latest complete list.\n",
        "                with open(checkpoint_path, 'w') as f:\n",
        "                    # Write each result dictionary as a new line.\n",
        "                    for res in results:\n",
        "                        f.write(json.dumps(res) + '\\n')\n",
        "            except IOError as e:\n",
        "                # Log an error if saving fails but continue the simulation.\n",
        "                print(f\"Warning: Could not write to checkpoint file {checkpoint_path}. Error: {e}\")\n",
        "\n",
        "    # --- Step 6: Finalization ---\n",
        "    # Perform a final save to ensure all results are written to disk.\n",
        "    try:\n",
        "        with open(checkpoint_path, 'w') as f:\n",
        "            for res in results:\n",
        "                f.write(json.dumps(res) + '\\n')\n",
        "        # Optionally, rename the file to signify completion.\n",
        "        completed_path = checkpoint_path.replace('.jsonl', '.completed.jsonl')\n",
        "        if os.path.exists(completed_path):\n",
        "             os.remove(completed_path) # remove old completed file if it exists\n",
        "        os.rename(checkpoint_path, completed_path)\n",
        "        print(f\"Condition complete. Final results saved to {completed_path}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error: Failed to save final results to {checkpoint_path}. Error: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_full_simulation(\n",
        "    personas_df: pd.DataFrame,\n",
        "    benchmark_df: pd.DataFrame,\n",
        "    configs: Dict[str, Any],\n",
        "    infrastructure: Dict[str, Any],\n",
        "    checkpoint_dir: str,\n",
        "    force_rerun: bool = False,\n",
        ") -> Dict[Tuple[str, str, str], List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of the entire 3x3x2 factorial simulation with\n",
        "    intelligent checkpoint management.\n",
        "\n",
        "    This master function iterates through every combination of model, persona\n",
        "    configuration, and role. For each of the 18 conditions, it first checks\n",
        "    if a completed result file already exists. If it does (and force_rerun is\n",
        "    False), it loads the result. Otherwise, it calls the robust, checkpoint-\n",
        "    enabled simulation function to generate the results.\n",
        "\n",
        "    Args:\n",
        "        personas_df: The cleaned and aligned DataFrame of persona data.\n",
        "        benchmark_df: The cleaned and aligned DataFrame of benchmark data.\n",
        "        configs: The main study configuration dictionary.\n",
        "        infrastructure: The dictionary containing initialized clients.\n",
        "        checkpoint_dir: The master directory to store all checkpoint and\n",
        "                        completed result files.\n",
        "        force_rerun: If True, all simulations will be re-run even if\n",
        "                     completed result files exist.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are tuples representing the experimental\n",
        "        condition (model_id, persona_config, role) and values are the lists\n",
        "        of simulation results for that condition.\n",
        "    \"\"\"\n",
        "    # Announce the start of the simulation task.\n",
        "    print(\"\\n--- Starting Full Simulation (Task 4) ---\")\n",
        "\n",
        "    # Retrieve the experimental factors from the configuration.\n",
        "    model_ids = configs[\"llm_and_prompt_parameters\"][\"model_identifiers\"]\n",
        "    persona_configs = configs[\"llm_and_prompt_parameters\"][\"persona_configurations\"]\n",
        "    roles = [\"proposer\", \"responder\"]\n",
        "\n",
        "    # Initialize a dictionary to hold all simulation results.\n",
        "    all_results = {}\n",
        "\n",
        "    # --- Factorial Design Implementation with Checkpoint Management ---\n",
        "    # Iterate through each experimental condition.\n",
        "    for model in model_ids:\n",
        "        for p_config in persona_configs:\n",
        "            for role in roles:\n",
        "                # Define the unique key for this experimental condition.\n",
        "                condition_key = (model, p_config, role)\n",
        "\n",
        "                # --- Step 3: Pre-computation Check Logic ---\n",
        "                # Construct the expected filename for a completed run of this condition.\n",
        "                condition_filename = f\"{model}_{p_config}_{role}.completed.jsonl\"\n",
        "                completed_path = os.path.join(checkpoint_dir, condition_filename)\n",
        "\n",
        "                # Check if the condition is already complete and we are not forcing a rerun.\n",
        "                if os.path.exists(completed_path) and not force_rerun:\n",
        "                    # If so, load the results directly from the completed file.\n",
        "                    print(f\"Condition {condition_key} already complete. Loading from file.\")\n",
        "                    try:\n",
        "                        # Open the completed file and load the results.\n",
        "                        with open(completed_path, 'r') as f:\n",
        "                            # Use a list comprehension for efficient loading of the JSONL file.\n",
        "                            loaded_results = [json.loads(line) for line in f]\n",
        "                        # Store the loaded results.\n",
        "                        all_results[condition_key] = loaded_results\n",
        "                        # Skip to the next condition in the loop.\n",
        "                        continue\n",
        "                    except (IOError, json.JSONDecodeError) as e:\n",
        "                        # If the completed file is corrupted, log a warning and proceed to run the simulation.\n",
        "                        print(f\"Warning: Could not load completed file {completed_path}. Re-running simulation. Error: {e}\")\n",
        "\n",
        "                # --- Step 4: Calling the Subordinate Function ---\n",
        "                # If the condition is not complete or a rerun is forced, execute the simulation.\n",
        "                condition_results = _run_single_simulation_condition(\n",
        "                    model_id=model,\n",
        "                    persona_config=p_config,\n",
        "                    role=role,\n",
        "                    personas_df=personas_df,\n",
        "                    benchmark_df=benchmark_df,\n",
        "                    configs=configs,\n",
        "                    infrastructure=infrastructure,\n",
        "                    checkpoint_dir=checkpoint_dir, # Pass the directory for the subordinate to manage its file.\n",
        "                )\n",
        "\n",
        "                # Store the newly computed results in the main dictionary.\n",
        "                all_results[condition_key] = condition_results\n",
        "\n",
        "    # Announce the completion of the simulation task.\n",
        "    print(\"\\n--- Full Simulation complete. ---\")\n",
        "\n",
        "    # Return the comprehensive dictionary of results.\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "kMXlfH46XcuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Data Aggregation and Statistical Processing\n",
        "\n",
        "def _aggregate_proposer_distributions(\n",
        "    all_results: Dict[Tuple[str, str, str], List[Dict[str, Any]]]\n",
        ") -> Dict[Tuple[str, str], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Aggregates raw proposer simulation results into probability mass functions.\n",
        "\n",
        "    This function processes the 9 proposer experimental conditions. For each\n",
        "    condition, it extracts the list of agent decisions (offers), handles any\n",
        "    failed decisions, and computes a complete probability mass function (PMF)\n",
        "    over the discrete support [0, 100].\n",
        "\n",
        "    Args:\n",
        "        all_results: The dictionary of raw simulation results from Task 4.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are (model_id, persona_config) tuples and\n",
        "        values are 101-element NumPy arrays representing the PMF for that\n",
        "        proposer condition.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the resulting PMFs.\n",
        "    proposer_distributions = {}\n",
        "\n",
        "    # Iterate through all result sets in the input dictionary.\n",
        "    for condition_key, results_list in all_results.items():\n",
        "        # Unpack the key to identify the condition.\n",
        "        model_id, persona_config, role = condition_key\n",
        "\n",
        "        # Process only the 'proposer' roles.\n",
        "        if role == \"proposer\":\n",
        "            # --- Step 5.1.1: Individual Decision Aggregation ---\n",
        "            # Extract valid decisions, filtering out any failures (decision is None).\n",
        "            decisions = [res[\"decision\"] for res in results_list if res[\"decision\"] is not None]\n",
        "\n",
        "            # If there are no valid decisions, create an empty distribution.\n",
        "            if not decisions:\n",
        "                pmf = np.zeros(101, dtype=float)\n",
        "            else:\n",
        "                # Use np.bincount to efficiently count frequencies of each offer.\n",
        "                # minlength ensures the output array has length 101, covering all\n",
        "                # possible offers from 0 to 100.\n",
        "                frequencies = np.bincount(decisions, minlength=101)\n",
        "\n",
        "                # Normalize frequencies to get the probability mass function.\n",
        "                # P(X = k) = f(k) / N\n",
        "                total_valid_decisions = len(decisions)\n",
        "                pmf = frequencies / total_valid_decisions\n",
        "\n",
        "            # Store the PMF in the output dictionary.\n",
        "            proposer_distributions[(model_id, persona_config)] = pmf\n",
        "\n",
        "    return proposer_distributions\n",
        "\n",
        "\n",
        "def _aggregate_responder_acceptance_rates(\n",
        "    all_results: Dict[Tuple[str, str, str], List[Dict[str, Any]]]\n",
        ") -> Dict[Tuple[str, str], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Aggregates raw responder simulation results into acceptance rate functions.\n",
        "\n",
        "    This function processes the 9 responder experimental conditions. For each\n",
        "    condition, it converts the results into a DataFrame, maps decisions to a\n",
        "    binary format (accept=1, reject=0), and then calculates the acceptance\n",
        "    rate and sample size for each unique offer amount presented to the agents.\n",
        "\n",
        "    Args:\n",
        "        all_results: The dictionary of raw simulation results from Task 4.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are (model_id, persona_config) tuples and\n",
        "        values are pandas DataFrames. Each DataFrame is indexed by offer amount\n",
        "        (0-100) and has columns 'acceptance_rate' and 'sample_size'.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the resulting acceptance rate DataFrames.\n",
        "    responder_rates = {}\n",
        "\n",
        "    # Iterate through all result sets.\n",
        "    for condition_key, results_list in all_results.items():\n",
        "        # Unpack the key.\n",
        "        model_id, persona_config, role = condition_key\n",
        "\n",
        "        # Process only the 'responder' roles.\n",
        "        if role == \"responder\":\n",
        "            # --- Step 5.2.1: Acceptance Rate Calculation by Offer Level ---\n",
        "            # Convert the list of result dictionaries to a pandas DataFrame.\n",
        "            results_df = pd.DataFrame(results_list)\n",
        "\n",
        "            # Filter out failed decisions.\n",
        "            valid_results_df = results_df.dropna(subset=['decision', 'offer_amount'])\n",
        "\n",
        "            # Map string decisions to a binary integer format.\n",
        "            valid_results_df['decision_binary'] = valid_results_df['decision'].map({'accept': 1, 'reject': 0})\n",
        "\n",
        "            # Group by the offer amount and aggregate to get mean (rate) and count.\n",
        "            # A(k) = Σ I(decision_i = accept) / N_k for all i where offer_i = k\n",
        "            agg_rates = valid_results_df.groupby('offer_amount')['decision_binary'].agg(\n",
        "                acceptance_rate='mean',\n",
        "                sample_size='count'\n",
        "            )\n",
        "\n",
        "            # Reindex to ensure the DataFrame covers the full support [0, 100].\n",
        "            # Offers not present in the data will have NaN values.\n",
        "            full_range_index = pd.Index(range(101), name='offer_amount')\n",
        "            agg_rates = agg_rates.reindex(full_range_index)\n",
        "\n",
        "            # Store the resulting DataFrame.\n",
        "            responder_rates[(model_id, persona_config)] = agg_rates\n",
        "\n",
        "    return responder_rates\n",
        "\n",
        "\n",
        "def _prepare_benchmark_distributions(\n",
        "    benchmark_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Processes the empirical benchmark data into the same statistical formats\n",
        "    as the simulation data.\n",
        "\n",
        "    This function ensures a valid comparison by applying the exact same\n",
        "    aggregation logic to the benchmark data as is applied to the simulated\n",
        "    data. It computes the empirical PMF for proposer offers and the empirical\n",
        "    acceptance rate function for responder decisions.\n",
        "\n",
        "    Args:\n",
        "        benchmark_df: The cleaned and aligned DataFrame of benchmark data.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the processed benchmark distributions:\n",
        "        - 'proposer_pmf': A 101-element NumPy array.\n",
        "        - 'responder_rates': A pandas DataFrame of acceptance rates.\n",
        "    \"\"\"\n",
        "    # --- Step 5.3.1: Empirical Proposer Distribution ---\n",
        "    # Extract the proposer offers from the benchmark DataFrame.\n",
        "    proposer_offers = benchmark_df['proposer_offer']\n",
        "\n",
        "    # Calculate the frequency of each offer.\n",
        "    frequencies = np.bincount(proposer_offers, minlength=101)\n",
        "\n",
        "    # Normalize to get the empirical PMF.\n",
        "    # P_human(X = k) = count(offer = k) / 1000\n",
        "    empirical_proposer_pmf = frequencies / len(proposer_offers)\n",
        "\n",
        "    # --- Step 5.3.2: Empirical Responder Acceptance Rates ---\n",
        "    # Create a copy to avoid modifying the original DataFrame.\n",
        "    df = benchmark_df.copy()\n",
        "\n",
        "    # Map string decisions to binary format.\n",
        "    df['decision_binary'] = df['responder_decision'].map({'accept': 1, 'reject': 0})\n",
        "\n",
        "    # Group by offer and aggregate to get acceptance rates and sample sizes.\n",
        "    empirical_responder_rates = df.groupby('proposer_offer')['decision_binary'].agg(\n",
        "        acceptance_rate='mean',\n",
        "        sample_size='count'\n",
        "    )\n",
        "\n",
        "    # Reindex to the full support [0, 100].\n",
        "    full_range_index = pd.Index(range(101), name='offer_amount')\n",
        "    empirical_responder_rates = empirical_responder_rates.reindex(full_range_index)\n",
        "\n",
        "    # Package the benchmark distributions into a dictionary.\n",
        "    return {\n",
        "        \"proposer_pmf\": empirical_proposer_pmf,\n",
        "        \"responder_rates\": empirical_responder_rates\n",
        "    }\n",
        "\n",
        "\n",
        "def aggregate_and_process_data(\n",
        "    all_simulation_results: Dict[Tuple[str, str, str], List[Dict[str, Any]]],\n",
        "    benchmark_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the aggregation and statistical processing of all simulation\n",
        "    and benchmark data.\n",
        "\n",
        "    This master function takes the raw outputs from the simulation engine and\n",
        "    the benchmark data, and transforms them into analysis-ready statistical\n",
        "    objects. It systematically processes all 18 simulated conditions and the\n",
        "    empirical data, ensuring consistent formatting throughout.\n",
        "\n",
        "    Args:\n",
        "        all_simulation_results: The dictionary of raw simulation results from Task 4.\n",
        "        benchmark_df: The cleaned and aligned DataFrame of benchmark data.\n",
        "\n",
        "    Returns:\n",
        "        A comprehensive dictionary containing all processed data:\n",
        "        - 'proposer_distributions': PMFs for all 9 simulated proposer conditions.\n",
        "        - 'responder_rates': Acceptance rate functions for all 9 responder conditions.\n",
        "        - 'benchmark_distributions': The processed empirical distributions.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Aggregating and Processing Data (Task 5) ---\")\n",
        "\n",
        "    # Step 1: Aggregate proposer simulation results.\n",
        "    print(\"Step 5.1: Aggregating proposer distributions...\")\n",
        "    proposer_dists = _aggregate_proposer_distributions(all_simulation_results)\n",
        "    print(\"...Proposer aggregation complete.\")\n",
        "\n",
        "    # Step 2: Aggregate responder simulation results.\n",
        "    print(\"\\nStep 5.2: Aggregating responder acceptance rates...\")\n",
        "    responder_rates_data = _aggregate_responder_acceptance_rates(all_simulation_results)\n",
        "    print(\"...Responder aggregation complete.\")\n",
        "\n",
        "    # Step 3: Prepare benchmark distributions using the same logic.\n",
        "    print(\"\\nStep 5.3: Preparing benchmark distributions...\")\n",
        "    benchmark_dists = _prepare_benchmark_distributions(benchmark_df)\n",
        "    print(\"...Benchmark preparation complete.\")\n",
        "\n",
        "    # Combine all processed data into a single analysis-ready dictionary.\n",
        "    analysis_ready_data = {\n",
        "        \"proposer_distributions\": proposer_dists,\n",
        "        \"responder_rates\": responder_rates_data,\n",
        "        \"benchmark_distributions\": benchmark_dists\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Data Aggregation and Processing complete. ---\")\n",
        "    return analysis_ready_data\n"
      ],
      "metadata": {
        "id": "bB67xZWBYS9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Wasserstein Distance Computation and Validation\n",
        "\n",
        "def _calculate_wasserstein_distance(\n",
        "    dist_a: Union[np.ndarray, pd.Series],\n",
        "    dist_b: Union[np.ndarray, pd.Series],\n",
        "    role: str,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Wasserstein-1 distance between two distributions.\n",
        "\n",
        "    This function provides a unified interface for calculating the distance for\n",
        "    both proposer PMFs and responder acceptance rate functions. It handles the\n",
        "    specific preprocessing required for each case before calling the SciPy\n",
        "    implementation.\n",
        "\n",
        "    Args:\n",
        "        dist_a: The first distribution (NumPy array for PMF, pandas Series for rates).\n",
        "        dist_b: The second distribution (NumPy array for PMF, pandas Series for rates).\n",
        "        role: The agent's role ('proposer' or 'responder'), which determines\n",
        "              the calculation method.\n",
        "\n",
        "    Returns:\n",
        "        The calculated raw Wasserstein-1 distance as a float.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an invalid role is provided or inputs are unsuitable.\n",
        "    \"\"\"\n",
        "    # --- Step 6.1.1: Distance Metric Calculation Framework ---\n",
        "    if role == \"proposer\":\n",
        "        # For proposers, distributions are PMFs (weights). The values are the offers [0, 100].\n",
        "        # Ensure inputs are NumPy arrays.\n",
        "        if not isinstance(dist_a, np.ndarray) or not isinstance(dist_b, np.ndarray):\n",
        "            raise ValueError(\"Proposer distributions must be NumPy arrays.\")\n",
        "\n",
        "        # Define the values (offer amounts 0-100).\n",
        "        values = np.arange(len(dist_a))\n",
        "\n",
        "        # Calculate the Wasserstein distance using the PMFs as weights.\n",
        "        # W₁(P, Q) = Σ |CDF_P(i) - CDF_Q(i)|\n",
        "        return wasserstein_distance(values, values, u_weights=dist_a, v_weights=dist_b)\n",
        "\n",
        "    elif role == \"responder\":\n",
        "        # For responders, distributions are acceptance rate functions.\n",
        "        # Ensure inputs are pandas Series.\n",
        "        if not isinstance(dist_a, pd.Series) or not isinstance(dist_b, pd.Series):\n",
        "            raise ValueError(\"Responder distributions must be pandas Series.\")\n",
        "\n",
        "        # --- Preprocessing for Responder Rates ---\n",
        "        # Create copies to avoid modifying original data.\n",
        "        rate_a = dist_a.copy()\n",
        "        rate_b = dist_b.copy()\n",
        "\n",
        "        # Handle NaN values by linear interpolation. This creates continuous\n",
        "        # functions from the sparse acceptance rate data.\n",
        "        rate_a.interpolate(method='linear', inplace=True)\n",
        "        rate_b.interpolate(method='linear', inplace=True)\n",
        "\n",
        "        # Fill any remaining NaNs at the start/end with the nearest valid value.\n",
        "        rate_a.fillna(method='bfill', inplace=True)\n",
        "        rate_a.fillna(method='ffill', inplace=True)\n",
        "        rate_b.fillna(method='bfill', inplace=True)\n",
        "        rate_b.fillna(method='ffill', inplace=True)\n",
        "\n",
        "        # The distance is calculated between the two vectors of acceptance rates.\n",
        "        # This can be seen as a Wasserstein distance between two discrete uniform\n",
        "        # distributions over the values in the vectors.\n",
        "        return wasserstein_distance(rate_a.values, rate_b.values)\n",
        "\n",
        "    else:\n",
        "        # Raise an error for an unrecognized role.\n",
        "        raise ValueError(f\"Invalid role for distance calculation: '{role}'\")\n",
        "\n",
        "\n",
        "def _compute_all_distances(\n",
        "    analysis_data: Dict[str, Any]\n",
        ") -> Dict[Tuple[str, str, str], float]:\n",
        "    \"\"\"\n",
        "    Computes Wasserstein distances for all 18 experimental conditions against\n",
        "    the benchmark.\n",
        "\n",
        "    This function iterates through every simulated distribution (9 proposer,\n",
        "    9 responder) and calculates its distance to the corresponding empirical\n",
        "    benchmark distribution.\n",
        "\n",
        "    Args:\n",
        "        analysis_data: The dictionary of analysis-ready data from Task 5.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are (model, persona, role) tuples and values\n",
        "        are the raw computed Wasserstein distances.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the raw distance results.\n",
        "    raw_distances = {}\n",
        "\n",
        "    # Retrieve the processed benchmark distributions.\n",
        "    benchmark_dists = analysis_data[\"benchmark_distributions\"]\n",
        "    benchmark_proposer_pmf = benchmark_dists[\"proposer_pmf\"]\n",
        "    benchmark_responder_rates = benchmark_dists[\"responder_rates\"][\"acceptance_rate\"]\n",
        "\n",
        "    # --- Step 6.1.3: Systematic Distance Matrix Construction (Calculation part) ---\n",
        "    # Calculate distances for proposer conditions.\n",
        "    for (model, persona), pmf in analysis_data[\"proposer_distributions\"].items():\n",
        "        distance = _calculate_wasserstein_distance(pmf, benchmark_proposer_pmf, \"proposer\")\n",
        "        raw_distances[(model, persona, \"Proposer\")] = distance\n",
        "\n",
        "    # Calculate distances for responder conditions.\n",
        "    for (model, persona), rates_df in analysis_data[\"responder_rates\"].items():\n",
        "        sim_rates = rates_df[\"acceptance_rate\"]\n",
        "        distance = _calculate_wasserstein_distance(sim_rates, benchmark_responder_rates, \"responder\")\n",
        "        raw_distances[(model, persona, \"Responder\")] = distance\n",
        "\n",
        "    return raw_distances\n",
        "\n",
        "\n",
        "def _format_results_table(\n",
        "    raw_distances: Dict[Tuple[str, str, str], float],\n",
        "    configs: Dict[str, Any]\n",
        ") -> Styler:\n",
        "    \"\"\"\n",
        "    Formats computed distances into a publication-ready styled table that\n",
        "    replicates Table 2 from the paper.\n",
        "\n",
        "    This function takes the raw Wasserstein distance values, applies the\n",
        "    specified scaling factor, and pivots the data into a wide-format DataFrame.\n",
        "    It then uses the pandas Styler API to apply number formatting (3 decimal\n",
        "    places) and to highlight the minimum distance within each model's group\n",
        "    by bolding the text.\n",
        "\n",
        "    Args:\n",
        "        raw_distances: A dictionary of raw Wasserstein distances for all conditions.\n",
        "        configs: The main study configuration dictionary, for the scaling factor\n",
        "                 and configuration order.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Styler object containing the formatted and styled table,\n",
        "        ready for display or export.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Core Data Structuring ---\n",
        "    # Get the scaling factor from the configuration.\n",
        "    scaling_factor = configs[\"analysis_parameters\"][\"wasserstein_distance_scaling_factor\"]\n",
        "\n",
        "    # Convert the dictionary of distances to a pandas Series for easy manipulation.\n",
        "    distances_series = pd.Series(raw_distances)\n",
        "\n",
        "    # Apply the scaling factor as specified in the paper.\n",
        "    scaled_distances = distances_series * scaling_factor\n",
        "\n",
        "    # Reset the index to turn the multi-level index into columns.\n",
        "    df = scaled_distances.reset_index()\n",
        "    df.columns = [\"Model\", \"Persona\", \"Role\", \"Distance\"]\n",
        "\n",
        "    # Pivot the table to create the desired wide format.\n",
        "    # Index by Model and Persona, with Role as columns.\n",
        "    results_table = df.pivot_table(\n",
        "        index=[\"Model\", \"Persona\"],\n",
        "        columns=\"Role\",\n",
        "        values=\"Distance\"\n",
        "    )\n",
        "\n",
        "    # Ensure the persona configurations are in the correct order for each model.\n",
        "    persona_order = configs[\"llm_and_prompt_parameters\"][\"persona_configurations\"]\n",
        "    model_order = configs[\"llm_and_prompt_parameters\"][\"model_identifiers\"]\n",
        "    results_table = results_table.reindex(model_order, level=\"Model\")\n",
        "    results_table = results_table.reindex(persona_order, level=\"Persona\")\n",
        "\n",
        "    # --- Step 2 & 3: Styling Logic Implementation ---\n",
        "    # Create a Styler object from the DataFrame.\n",
        "    styler = results_table.style\n",
        "\n",
        "    # --- Step 4: Apply Number Formatting ---\n",
        "    # Apply number formatting to display all distances with 3 decimal places.\n",
        "    styler.format(\"{:.3f}\")\n",
        "\n",
        "    # --- Step 3: Apply highlight_min Logic ---\n",
        "    # Define a function to apply bolding to the minimum value in a Series.\n",
        "    def highlight_min(series: pd.Series) -> List[str]:\n",
        "        # Identify the minimum value in the series.\n",
        "        is_min = series == series.min()\n",
        "        # Return a list of CSS styles: 'font-weight: bold' for the minimum, '' otherwise.\n",
        "        return ['font-weight: bold' if v else '' for v in is_min]\n",
        "\n",
        "    # Apply the highlighting function to each model's subgroup for each column.\n",
        "    # This ensures we find the minimum *within* each model's set of results.\n",
        "    styler.apply(highlight_min, axis=0, subset=pd.IndexSlice[model_order[0], :])\n",
        "    styler.apply(highlight_min, axis=0, subset=pd.IndexSlice[model_order[1], :])\n",
        "    styler.apply(highlight_min, axis=0, subset=pd.IndexSlice[model_order[2], :])\n",
        "\n",
        "    # Set a caption for the table.\n",
        "    styler.set_caption(\"Wasserstein Distances (scaled by 10⁻²) Between Simulated and Human Behavior\")\n",
        "\n",
        "    # --- Step 5: Return the Styler Object ---\n",
        "    return styler\n",
        "\n",
        "\n",
        "def compute_and_validate_results(\n",
        "    analysis_ready_data: Dict[str, Any],\n",
        "    study_configurations: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the Wasserstein distance computation and result formatting.\n",
        "\n",
        "    This master function for Task 6 takes the aggregated statistical data,\n",
        "    computes the Wasserstein distance for every experimental condition against\n",
        "    the benchmark, and formats the final results into a table that replicates\n",
        "    Table 2 from the source paper.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_data: The dictionary of processed data from Task 5.\n",
        "        study_configurations: The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the final scaled Wasserstein distances,\n",
        "        formatted for presentation and analysis.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Computing and Validating Results (Task 6) ---\")\n",
        "\n",
        "    # Step 1: Compute all raw Wasserstein distances.\n",
        "    print(\"Step 6.1: Computing Wasserstein distances for all 18 conditions...\")\n",
        "    raw_distances = _compute_all_distances(analysis_ready_data)\n",
        "    print(\"...Distance computation complete.\")\n",
        "\n",
        "    # Step 2: Format the results into the final table.\n",
        "    print(\"\\nStep 6.2: Formatting results into final table...\")\n",
        "    final_results_table = _format_results_table(raw_distances, study_configurations)\n",
        "    print(\"...Results table formatted successfully.\")\n",
        "\n",
        "    print(\"\\n--- Quantitative Validation complete. ---\")\n",
        "    return final_results_table\n"
      ],
      "metadata": {
        "id": "jOkQMwO1ZBSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Results Visualization and Documentation\n",
        "\n",
        "def _plot_proposer_distributions(\n",
        "    analysis_data: Dict[str, Any],\n",
        "    configs: Dict[str, Any],\n",
        "    output_path: str\n",
        ") -> Tuple[Figure, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates and saves the proposer distribution plot, replicating Figure 3.\n",
        "\n",
        "    This function creates a three-panel figure, one for each LLM, comparing the\n",
        "    simulated proposer offer distributions against the empirical benchmark.\n",
        "    The benchmark is shown as black bars, and the three persona configuration\n",
        "    results are overlaid as colored lines with distinct styles.\n",
        "\n",
        "    Args:\n",
        "        analysis_data: The dictionary of analysis-ready data from Task 5.\n",
        "        configs: The main study configuration dictionary.\n",
        "        output_path: The file path (e.g., 'figure3.png') to save the plot.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the matplotlib Figure and Axes objects.\n",
        "    \"\"\"\n",
        "    # --- Step 7.1.1: Proposer Distribution Visualization (Figure 3) ---\n",
        "    # Retrieve model and persona configuration order.\n",
        "    model_ids = configs[\"llm_and_prompt_parameters\"][\"model_identifiers\"]\n",
        "    persona_configs = configs[\"llm_and_prompt_parameters\"][\"persona_configurations\"]\n",
        "\n",
        "    # Define plotting styles to match the paper.\n",
        "    styles = {\n",
        "        \"no_persona\": {\"color\": \"darkorange\", \"linestyle\": \"--\", \"label\": \"nothing\"},\n",
        "        \"6_traits\": {\"color\": \"blue\", \"linestyle\": \"-\", \"label\": \"6 traits\"},\n",
        "        \"21_traits\": {\"color\": \"green\", \"linestyle\": \"-\", \"label\": \"21 traits\"},\n",
        "    }\n",
        "\n",
        "    # Create a 1x3 subplot figure.\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
        "    fig.suptitle(\"Distribution of Offers Proposed by LLM Agents\", fontsize=16)\n",
        "\n",
        "    # Retrieve the benchmark distribution.\n",
        "    benchmark_pmf = analysis_data[\"benchmark_distributions\"][\"proposer_pmf\"]\n",
        "    offer_values = np.arange(len(benchmark_pmf))\n",
        "\n",
        "    # Iterate through each model to create its corresponding subplot.\n",
        "    for i, model_id in enumerate(model_ids):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Plot the empirical benchmark data as black bars.\n",
        "        ax.bar(offer_values, benchmark_pmf, color='black', alpha=0.6, width=1.0, label='raw data')\n",
        "\n",
        "        # Overlay the three simulated distributions for this model.\n",
        "        for p_config in persona_configs:\n",
        "            # Retrieve the simulated PMF.\n",
        "            sim_pmf = analysis_data[\"proposer_distributions\"].get((model_id, p_config))\n",
        "            if sim_pmf is not None:\n",
        "                # Plot the PMF as a line plot.\n",
        "                style = styles[p_config]\n",
        "                ax.plot(offer_values, sim_pmf, color=style[\"color\"], linestyle=style[\"linestyle\"], label=style[\"label\"])\n",
        "\n",
        "        # --- Formatting ---\n",
        "        ax.set_title(model_id)\n",
        "        ax.set_xlabel(\"Offer\")\n",
        "        ax.set_xlim(-1, 101)\n",
        "        ax.set_ylim(bottom=0)\n",
        "        if i == 0:\n",
        "            ax.set_ylabel(\"Density\")\n",
        "\n",
        "    # Add a single legend for the entire figure.\n",
        "    handles, labels = axes[0].get_legend_handles_labels()\n",
        "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(1.0, 0.8))\n",
        "\n",
        "    # Adjust layout and save the figure.\n",
        "    plt.tight_layout(rect=[0, 0, 0.95, 1]) # Adjust for legend\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Proposer distribution plot saved to {output_path}\")\n",
        "\n",
        "    return fig, axes\n",
        "\n",
        "\n",
        "def _plot_responder_acceptance_rates(\n",
        "    analysis_data: Dict[str, Any],\n",
        "    configs: Dict[str, Any],\n",
        "    output_path: str\n",
        ") -> Tuple[Figure, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates and saves the responder acceptance rate plot, replicating Figure 4.\n",
        "\n",
        "    This function creates a three-panel bubble plot, one for each LLM. It shows\n",
        "    the acceptance rate as a function of the offer amount. The size of each\n",
        "    point (bubble) is proportional to the frequency of that offer in the\n",
        "    benchmark data.\n",
        "\n",
        "    Args:\n",
        "        analysis_data: The dictionary of analysis-ready data from Task 5.\n",
        "        configs: The main study configuration dictionary.\n",
        "        output_path: The file path (e.g., 'figure4.png') to save the plot.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the matplotlib Figure and Axes objects.\n",
        "    \"\"\"\n",
        "    # --- Step 7.1.2: Responder Acceptance Visualization (Figure 4) ---\n",
        "    # Retrieve model and persona configuration order.\n",
        "    model_ids = configs[\"llm_and_prompt_parameters\"][\"model_identifiers\"]\n",
        "    persona_configs = configs[\"llm_and_prompt_parameters\"][\"persona_configurations\"]\n",
        "\n",
        "    # Define plotting styles.\n",
        "    colors = {\n",
        "        \"no_persona\": \"darkorange\",\n",
        "        \"6_traits\": \"blue\",\n",
        "        \"21_traits\": \"green\",\n",
        "    }\n",
        "\n",
        "    # Create a 1x3 subplot figure.\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
        "    fig.suptitle(\"Acceptance Rates of LLM Agents in the Responder Role\", fontsize=16)\n",
        "\n",
        "    # Retrieve the benchmark data.\n",
        "    benchmark_rates_df = analysis_data[\"benchmark_distributions\"][\"responder_rates\"]\n",
        "\n",
        "    # Define a scaling factor for bubble sizes for better visibility.\n",
        "    bubble_scale_factor = 10.0\n",
        "\n",
        "    # Iterate through each model to create its subplot.\n",
        "    for i, model_id in enumerate(model_ids):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Plot the empirical benchmark data as black points.\n",
        "        # The size is also scaled by frequency.\n",
        "        benchmark_plot_data = benchmark_rates_df.dropna()\n",
        "        ax.scatter(\n",
        "            benchmark_plot_data.index,\n",
        "            benchmark_plot_data['acceptance_rate'],\n",
        "            s=benchmark_plot_data['sample_size'] * bubble_scale_factor,\n",
        "            color='black',\n",
        "            alpha=0.6,\n",
        "            label='raw data'\n",
        "        )\n",
        "\n",
        "        # Overlay the three simulated distributions.\n",
        "        for p_config in persona_configs:\n",
        "            sim_rates_df = analysis_data[\"responder_rates\"].get((model_id, p_config))\n",
        "            if sim_rates_df is not None:\n",
        "                # Use the sample sizes from the benchmark data for bubble scaling.\n",
        "                plot_data = sim_rates_df.join(benchmark_rates_df['sample_size']).dropna()\n",
        "                ax.scatter(\n",
        "                    plot_data.index,\n",
        "                    plot_data['acceptance_rate'],\n",
        "                    s=plot_data['sample_size'] * bubble_scale_factor,\n",
        "                    color=colors[p_config],\n",
        "                    alpha=0.7,\n",
        "                    label=p_config.replace(\"_\", \" \")\n",
        "                )\n",
        "\n",
        "        # --- Formatting ---\n",
        "        ax.set_title(model_id)\n",
        "        ax.set_xlabel(\"Offer\")\n",
        "        ax.set_xlim(-1, 101)\n",
        "        ax.set_ylim(-0.05, 1.05)\n",
        "        if i == 0:\n",
        "            ax.set_ylabel(\"Acceptance rate\")\n",
        "\n",
        "    # Create a custom legend for colors, as bubble sizes vary.\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], marker='o', color='w', label='raw data', markerfacecolor='black', markersize=10),\n",
        "        Line2D([0], [0], marker='o', color='w', label='nothing', markerfacecolor=colors['no_persona'], markersize=10),\n",
        "        Line2D([0], [0], marker='o', color='w', label='6 traits', markerfacecolor=colors['6_traits'], markersize=10),\n",
        "        Line2D([0], [0], marker='o', color='w', label='21 traits', markerfacecolor=colors['21_traits'], markersize=10),\n",
        "    ]\n",
        "    fig.legend(handles=legend_elements, loc='center right', bbox_to_anchor=(1.0, 0.8))\n",
        "\n",
        "    # Adjust layout and save the figure.\n",
        "    plt.tight_layout(rect=[0, 0, 0.95, 1])\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Responder acceptance rate plot saved to {output_path}\")\n",
        "\n",
        "    return fig, axes\n",
        "\n",
        "\n",
        "def generate_visualizations(\n",
        "    analysis_ready_data: Dict[str, Any],\n",
        "    study_configurations: Dict[str, Any],\n",
        "    output_dir: str = \".\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of all key figures from the paper.\n",
        "\n",
        "    This master function for Task 7 calls dedicated plotting functions to\n",
        "    replicate Figure 3 (Proposer Distributions) and Figure 4 (Responder\n",
        "    Acceptance Rates), saving them to the specified directory.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_data: The dictionary of processed data from Task 5.\n",
        "        study_configurations: The main study configuration dictionary.\n",
        "        output_dir: The directory where the plot images will be saved.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Generating Visualizations (Task 7) ---\")\n",
        "\n",
        "    # Ensure the output directory exists.\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Generate and save the proposer distribution plot (Figure 3).\n",
        "    proposer_plot_path = os.path.join(output_dir, \"figure3_proposer_distributions.png\")\n",
        "    _plot_proposer_distributions(analysis_ready_data, study_configurations, proposer_plot_path)\n",
        "\n",
        "    # Generate and save the responder acceptance rate plot (Figure 4).\n",
        "    responder_plot_path = os.path.join(output_dir, \"figure4_responder_acceptance_rates.png\")\n",
        "    _plot_responder_acceptance_rates(analysis_ready_data, study_configurations, responder_plot_path)\n",
        "\n",
        "    plt.show() # Display plots if in an interactive environment.\n",
        "\n",
        "    print(\"\\n--- Visualization generation complete. ---\")\n"
      ],
      "metadata": {
        "id": "rTXXbd1GZ-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Research Pipeline Orchestration and Robustness Analysis\n",
        "\n",
        "def execute_bias_adjusted_llm_study(\n",
        "    study_configurations: Dict[str, Any],\n",
        "    output_dir: str,\n",
        "    personas_df: Optional[pd.DataFrame] = None,\n",
        "    benchmark_df: Optional[pd.DataFrame] = None,\n",
        "    personas_data_path: Optional[str] = None,\n",
        "    benchmark_data_path: Optional[str] = None,\n",
        "    force_rerun_simulation: bool = False,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end research pipeline for the study with\n",
        "    flexible data input methods.\n",
        "\n",
        "    This master orchestrator function manages the entire workflow. It can be\n",
        "    invoked either by providing file paths to the raw data (for standalone runs)\n",
        "    or by passing pre-loaded pandas DataFrames (for integration into larger\n",
        "    analytical workflows like sensitivity analysis).\n",
        "\n",
        "    The pipeline stages are:\n",
        "    1.  Load or receive raw data.\n",
        "    2.  Validate all inputs.\n",
        "    3.  Preprocess and cleanse data.\n",
        "    4.  Set up LLM API infrastructure.\n",
        "    5.  Run the full simulation with checkpointing.\n",
        "    6.  Aggregate results into statistical distributions.\n",
        "    7.  Compute quantitative validation metrics.\n",
        "    8.  Generate and save final visualizations.\n",
        "\n",
        "    Args:\n",
        "        study_configurations: The main study configuration dictionary.\n",
        "        output_dir: A directory to save all outputs (checkpoints, plots).\n",
        "        personas_df: A pre-loaded DataFrame of persona data.\n",
        "        benchmark_df: A pre-loaded DataFrame of benchmark data.\n",
        "        personas_data_path: File path to the raw personas CSV data.\n",
        "        benchmark_data_path: File path to the raw benchmark CSV data.\n",
        "        force_rerun_simulation: If True, ignores completed simulation files\n",
        "                                and re-runs all 18 conditions.\n",
        "\n",
        "    Returns:\n",
        "        A comprehensive dictionary containing the key artifacts from each\n",
        "        stage of the pipeline.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If neither a DataFrame nor a valid file path is provided\n",
        "                    for either of the required datasets.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Setup and Logging ---\n",
        "    # Announce the start of the pipeline.\n",
        "    print(\"--- EXECUTING END-TO-END RESEARCH PIPELINE ---\")\n",
        "\n",
        "    # Ensure the main output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize a dictionary to store all pipeline artifacts.\n",
        "    pipeline_artifacts = {}\n",
        "\n",
        "    try:\n",
        "        # --- Step 2: Flexible Data Loading ---\n",
        "        print(\"\\n[Stage 1/8] Loading raw data...\")\n",
        "\n",
        "        # --- Input Loading and Validation Logic for Personas Data ---\n",
        "        # If a DataFrame is not passed directly, load it from the provided path.\n",
        "        if personas_df is None:\n",
        "            # Check if a path was provided as an alternative.\n",
        "            if personas_data_path and os.path.exists(personas_data_path):\n",
        "                # Load the dataset from the CSV file.\n",
        "                personas_df = pd.read_csv(personas_data_path)\n",
        "            else:\n",
        "                # If neither is available, the pipeline cannot proceed.\n",
        "                raise ValueError(\"Must provide either a `personas_df` DataFrame or a valid `personas_data_path`.\")\n",
        "\n",
        "        # --- Input Loading and Validation Logic for Benchmark Data ---\n",
        "        # If a DataFrame is not passed directly, load it from the provided path.\n",
        "        if benchmark_df is None:\n",
        "            # Check if a path was provided as an alternative.\n",
        "            if benchmark_data_path and os.path.exists(benchmark_data_path):\n",
        "                # Load the dataset from the CSV file.\n",
        "                benchmark_df = pd.read_csv(benchmark_data_path)\n",
        "            else:\n",
        "                # If neither is available, the pipeline cannot proceed.\n",
        "                raise ValueError(\"Must provide either a `benchmark_df` DataFrame or a valid `benchmark_data_path`.\")\n",
        "\n",
        "        print(\"...Data loading complete.\")\n",
        "\n",
        "        # --- Step 3: Task 1 - Validation ---\n",
        "        print(\"\\n[Stage 2/8] Validating all study inputs...\")\n",
        "        # Validate the integrity and schema of all inputs. Halts on failure.\n",
        "        validate_study_inputs(personas_df, benchmark_df, study_configurations)\n",
        "        print(\"...Input validation successful.\")\n",
        "\n",
        "        # --- Step 4: Task 2 - Preprocessing ---\n",
        "        print(\"\\n[Stage 3/8] Preprocessing and cleansing data...\")\n",
        "        # Cleanse and align the data for the simulation.\n",
        "        clean_personas_df, clean_benchmark_df, cleansing_log = preprocess_and_cleanse_data(\n",
        "            personas_df, benchmark_df, study_configurations\n",
        "        )\n",
        "        # Store the cleansed data and logs.\n",
        "        pipeline_artifacts[\"clean_personas_df\"] = clean_personas_df\n",
        "        pipeline_artifacts[\"clean_benchmark_df\"] = clean_benchmark_df\n",
        "        pipeline_artifacts[\"cleansing_log\"] = cleansing_log\n",
        "        print(\"...Data preprocessing complete.\")\n",
        "\n",
        "        # --- Step 5: Task 3 - Infrastructure Setup ---\n",
        "        print(\"\\n[Stage 4/8] Setting up LLM infrastructure...\")\n",
        "        # Initialize and authenticate all required LLM API clients.\n",
        "        infrastructure = setup_llm_infrastructure(study_configurations)\n",
        "        pipeline_artifacts[\"infrastructure\"] = infrastructure\n",
        "        print(\"...LLM infrastructure setup complete.\")\n",
        "\n",
        "        # --- Step 6: Task 4 - Simulation ---\n",
        "        print(\"\\n[Stage 5/8] Running full simulation...\")\n",
        "        # Define the directory for simulation checkpoints.\n",
        "        checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n",
        "        # Execute the full 18-condition simulation.\n",
        "        all_simulation_results = run_full_simulation(\n",
        "            personas_df=clean_personas_df,\n",
        "            benchmark_df=clean_benchmark_df,\n",
        "            configs=study_configurations,\n",
        "            infrastructure=infrastructure,\n",
        "            checkpoint_dir=checkpoint_dir,\n",
        "            force_rerun=force_rerun_simulation,\n",
        "        )\n",
        "        pipeline_artifacts[\"raw_simulation_results\"] = all_simulation_results\n",
        "        print(\"...Full simulation complete.\")\n",
        "\n",
        "        # --- Step 7: Task 5 - Aggregation ---\n",
        "        print(\"\\n[Stage 6/8] Aggregating and processing results...\")\n",
        "        # Aggregate raw results into statistical distributions.\n",
        "        analysis_ready_data = aggregate_and_process_data(\n",
        "            all_simulation_results, clean_benchmark_df\n",
        "        )\n",
        "        pipeline_artifacts[\"analysis_ready_data\"] = analysis_ready_data\n",
        "        print(\"...Data aggregation complete.\")\n",
        "\n",
        "        # --- Step 8: Task 6 - Quantitative Validation ---\n",
        "        print(\"\\n[Stage 7/8] Computing quantitative validation metrics...\")\n",
        "        # Compute distances and get the styled table object.\n",
        "        results_styler = _format_results_table(\n",
        "            _compute_all_distances(analysis_ready_data), study_configurations\n",
        "        )\n",
        "        # Store both the raw data and the styled object.\n",
        "        pipeline_artifacts[\"results_table_data\"] = results_styler.data\n",
        "        pipeline_artifacts[\"results_table_styled\"] = results_styler\n",
        "        print(\"...Quantitative validation complete. Results table generated.\")\n",
        "        # Display the styled table for the user in an interactive environment.\n",
        "        display(results_styler)\n",
        "\n",
        "        # --- Step 9: Task 7 - Visualization ---\n",
        "        print(\"\\n[Stage 8/8] Generating final visualizations...\")\n",
        "        # Define the directory for plot outputs.\n",
        "        plots_dir = os.path.join(output_dir, \"plots\")\n",
        "        # Generate and save all required figures.\n",
        "        generate_visualizations(analysis_ready_data, study_configurations, plots_dir)\n",
        "        print(\"...Visualization generation complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception during the pipeline execution.\n",
        "        print(f\"\\n--- PIPELINE EXECUTION FAILED ---\")\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        # Re-raise the exception to provide a full traceback for debugging.\n",
        "        raise\n",
        "\n",
        "    # --- Step 10: Finalization and Return ---\n",
        "    # Announce the successful completion of the entire pipeline.\n",
        "    print(\"\\n--- PIPELINE EXECUTION COMPLETED SUCCESSFULLY ---\")\n",
        "\n",
        "    # Return the dictionary of all generated artifacts.\n",
        "    return pipeline_artifacts\n",
        "\n",
        "\n",
        "def run_sensitivity_analyses(\n",
        "    personas_df: pd.DataFrame,\n",
        "    benchmark_df: pd.DataFrame,\n",
        "    base_configs: Dict[str, Any],\n",
        "    base_output_dir: str,\n",
        "    sample_sizes_to_test: Optional[List[int]] = None,\n",
        "    alternative_trait_sets: Optional[Dict[str, List[str]]] = None,\n",
        "    alternative_prompt_templates: Optional[Dict[str, Dict[str, str]]] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes a series of sensitivity analyses by systematically varying key\n",
        "    parameters of the main research pipeline. (AMENDED FOR COMPATIBILITY)\n",
        "\n",
        "    This function serves as a meta-orchestrator, calling the main\n",
        "    `execute_bias_adjusted_llm_study` pipeline multiple times with modified\n",
        "    inputs to test the robustness of the findings. It supports sensitivity\n",
        "    testing for sample size, persona configuration, and prompt templates.\n",
        "    This version is updated to be compatible with the flexible, object-accepting\n",
        "    master orchestrator.\n",
        "\n",
        "    Args:\n",
        "        personas_df: The full, cleaned DataFrame of persona data.\n",
        "        benchmark_df: The full, cleaned DataFrame of benchmark data.\n",
        "        base_configs: The baseline study configuration dictionary.\n",
        "        base_output_dir: The root directory for storing all sensitivity run outputs.\n",
        "        sample_sizes_to_test: A list of sample sizes (e.g., [500, 750]) to test.\n",
        "        alternative_trait_sets: A dict where keys are names for alternative\n",
        "                                trait sets and values are lists of trait names.\n",
        "        alternative_prompt_templates: A dict where keys are names for prompt\n",
        "                                      variations and values are dicts containing\n",
        "                                      'proposer' and 'responder' templates.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the results tables from all sensitivity\n",
        "        analyses performed.\n",
        "    \"\"\"\n",
        "    # Announce the start of the sensitivity analysis task.\n",
        "    print(\"\\n--- EXECUTING SENSITIVITY ANALYSES (Task 8.2.1) ---\")\n",
        "\n",
        "    # Initialize a dictionary to store all sensitivity results.\n",
        "    sensitivity_results = {}\n",
        "\n",
        "    # --- 1. Sample Size Variation Analysis ---\n",
        "    if sample_sizes_to_test:\n",
        "        print(\"\\n[Sensitivity Analysis 1/3] Testing Sample Size Variation...\")\n",
        "        sample_size_results = {}\n",
        "        for size in sample_sizes_to_test:\n",
        "            print(f\"\\n--- Running pipeline for sample size: {size} ---\")\n",
        "            try:\n",
        "                # Create a reproducible random subsample of the data.\n",
        "                personas_sample = personas_df.sample(n=size, random_state=42)\n",
        "                # Align the benchmark data with the sampled participants.\n",
        "                benchmark_sample = benchmark_df[benchmark_df['interaction_id'].isin(personas_sample['participant_id'])].copy()\n",
        "\n",
        "                # Define a unique output directory for this run.\n",
        "                run_output_dir = os.path.join(base_output_dir, f\"sensitivity_sample_size_{size}\")\n",
        "\n",
        "                # Execute the full pipeline by passing the sampled DataFrames directly.\n",
        "                artifacts = execute_bias_adjusted_llm_study(\n",
        "                    study_configurations=base_configs,\n",
        "                    output_dir=run_output_dir,\n",
        "                    personas_df=personas_sample,\n",
        "                    benchmark_df=benchmark_sample,\n",
        "                    force_rerun_simulation=True,\n",
        "                )\n",
        "                # Store the final results table.\n",
        "                sample_size_results[size] = artifacts[\"results_table_data\"]\n",
        "            except Exception as e:\n",
        "                print(f\"Error during sample size {size} run: {e}\")\n",
        "                sample_size_results[size] = str(e)\n",
        "\n",
        "        sensitivity_results[\"sample_size_variation\"] = sample_size_results\n",
        "\n",
        "    # --- 2. Persona Configuration Sensitivity Analysis ---\n",
        "    if alternative_trait_sets:\n",
        "        print(\"\\n[Sensitivity Analysis 2/3] Testing Persona Configuration Variation...\")\n",
        "        persona_sens_results = {}\n",
        "        for name, trait_list in alternative_trait_sets.items():\n",
        "            print(f\"\\n--- Running pipeline for alternative trait set: {name} ---\")\n",
        "            try:\n",
        "                # Create a deep copy of the base configuration to modify it safely.\n",
        "                run_configs = copy.deepcopy(base_configs)\n",
        "                run_configs[\"llm_and_prompt_parameters\"][\"key_behavioral_indicators_6\"] = trait_list\n",
        "\n",
        "                # Define a unique output directory.\n",
        "                run_output_dir = os.path.join(base_output_dir, f\"sensitivity_persona_config_{name}\")\n",
        "\n",
        "                # Execute the pipeline with the modified config and the full datasets.\n",
        "                artifacts = execute_bias_adjusted_llm_study(\n",
        "                    study_configurations=run_configs,\n",
        "                    output_dir=run_output_dir,\n",
        "                    personas_df=personas_df,\n",
        "                    benchmark_df=benchmark_df,\n",
        "                    force_rerun_simulation=True,\n",
        "                )\n",
        "                # Store the resulting table.\n",
        "                persona_sens_results[name] = artifacts[\"results_table_data\"]\n",
        "            except Exception as e:\n",
        "                print(f\"Error during persona config '{name}' run: {e}\")\n",
        "                persona_sens_results[name] = str(e)\n",
        "\n",
        "        sensitivity_results[\"persona_config_variation\"] = persona_sens_results\n",
        "\n",
        "    # --- 3. Prompt Template Sensitivity Analysis ---\n",
        "    if alternative_prompt_templates:\n",
        "        print(\"\\n[Sensitivity Analysis 3/3] Testing Prompt Template Variation...\")\n",
        "        prompt_sens_results = {}\n",
        "        for name, template_dict in alternative_prompt_templates.items():\n",
        "            print(f\"\\n--- Running pipeline for prompt template: {name} ---\")\n",
        "            try:\n",
        "                # Create a deep copy of the configuration.\n",
        "                run_configs = copy.deepcopy(base_configs)\n",
        "                run_configs[\"llm_and_prompt_parameters\"][\"prompt_templates\"] = template_dict\n",
        "\n",
        "                # Define a unique output directory.\n",
        "                run_output_dir = os.path.join(base_output_dir, f\"sensitivity_prompt_template_{name}\")\n",
        "\n",
        "                # Execute the pipeline with the modified config and the full datasets.\n",
        "                artifacts = execute_bias_adjusted_llm_study(\n",
        "                    study_configurations=run_configs,\n",
        "                    output_dir=run_output_dir,\n",
        "                    personas_df=personas_df,\n",
        "                    benchmark_df=benchmark_df,\n",
        "                    force_rerun_simulation=True,\n",
        "                )\n",
        "                # Store the result.\n",
        "                prompt_sens_results[name] = artifacts[\"results_table_data\"]\n",
        "            except Exception as e:\n",
        "                print(f\"Error during prompt template '{name}' run: {e}\")\n",
        "                prompt_sens_results[name] = str(e)\n",
        "\n",
        "        sensitivity_results[\"prompt_template_variation\"] = prompt_sens_results\n",
        "\n",
        "    # Announce completion.\n",
        "    print(\"\\n--- Sensitivity Analyses complete. ---\")\n",
        "\n",
        "    # Return all collected results.\n",
        "    return sensitivity_results\n",
        "\n",
        "\n",
        "def run_alternative_metric_validation(\n",
        "    analysis_ready_data: Dict[str, Any],\n",
        "    configs: Dict[str, Any],\n",
        "    epsilon: float = 1e-12\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs validation using alternative distance/divergence metrics to test\n",
        "    the robustness of the primary findings.\n",
        "\n",
        "    This function recalculates the \"distance\" between the simulated and\n",
        "    empirical proposer distributions using three different metrics:\n",
        "    1.  Wasserstein-1 Distance (the primary metric from the paper).\n",
        "    2.  Kullback-Leibler (KL) Divergence.\n",
        "    3.  Jensen-Shannon (JS) Divergence.\n",
        "\n",
        "    By comparing the rankings of persona configurations across these different\n",
        "    metrics, we can assess whether the conclusion (e.g., that '21_traits' is\n",
        "    the best configuration for a given model) is robust to the specific choice\n",
        "    of metric. This analysis is performed only for the proposer role, as KL and\n",
        "    JS divergences are defined for probability distributions (PMFs).\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_data: The dictionary of analysis-ready data from Task 5.\n",
        "        configs: The main study configuration dictionary.\n",
        "        epsilon: A small value to add to probabilities for numerical stability\n",
        "                 in divergence calculations.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame comparing the performance of each experimental\n",
        "        condition across the three different metrics. The values are the raw\n",
        "        metric scores (lower is better).\n",
        "    \"\"\"\n",
        "    # Announce the start of the task.\n",
        "    print(\"\\n--- EXECUTING ALTERNATIVE IMPLEMENTATION VALIDATION (Task 8.2.2) ---\")\n",
        "\n",
        "    # Initialize a list to store the results of the analysis.\n",
        "    metric_results = []\n",
        "\n",
        "    # Retrieve the benchmark proposer distribution.\n",
        "    benchmark_pmf = analysis_ready_data[\"benchmark_distributions\"][\"proposer_pmf\"]\n",
        "\n",
        "    # --- Epsilon Smoothing for Divergence Metrics ---\n",
        "    # Add a small epsilon to the benchmark PMF and re-normalize to avoid log(0) or division by zero.\n",
        "    benchmark_pmf_smooth = benchmark_pmf + epsilon\n",
        "    benchmark_pmf_smooth /= np.sum(benchmark_pmf_smooth)\n",
        "\n",
        "    # --- Iterate through all proposer conditions ---\n",
        "    # Loop through each simulated proposer distribution.\n",
        "    for (model, persona), sim_pmf in analysis_ready_data[\"proposer_distributions\"].items():\n",
        "\n",
        "        # --- Epsilon Smoothing for Simulated Distribution ---\n",
        "        # Apply the same smoothing to the simulated PMF.\n",
        "        sim_pmf_smooth = sim_pmf + epsilon\n",
        "        sim_pmf_smooth /= np.sum(sim_pmf_smooth)\n",
        "\n",
        "        # --- Metric 1: Wasserstein Distance (Primary Metric) ---\n",
        "        # Calculate the primary metric for reference.\n",
        "        wasserstein_dist = _calculate_wasserstein_distance(sim_pmf, benchmark_pmf, \"proposer\")\n",
        "\n",
        "        # --- Metric 2: Kullback-Leibler Divergence ---\n",
        "        # D_KL(P || Q) = Σ P(i) * log(P(i) / Q(i))\n",
        "        # scipy.stats.entropy calculates this directly. We use P=simulated, Q=benchmark.\n",
        "        kl_div = entropy(pk=sim_pmf_smooth, qk=benchmark_pmf_smooth)\n",
        "\n",
        "        # --- Metric 3: Jensen-Shannon Divergence ---\n",
        "        # JSD is a symmetric, smoothed version of KL divergence.\n",
        "        # scipy.spatial.distance.jensenshannon returns the square root of the JSD.\n",
        "        js_dist = jensenshannon(p=sim_pmf_smooth, q=benchmark_pmf_smooth, base=2.0)\n",
        "\n",
        "        # Append the results for this condition to our list.\n",
        "        metric_results.append({\n",
        "            \"Model\": model,\n",
        "            \"Persona\": persona,\n",
        "            \"Wasserstein\": wasserstein_dist,\n",
        "            \"KL_Divergence\": kl_div,\n",
        "            \"JS_Distance\": js_dist,\n",
        "        })\n",
        "\n",
        "    # --- Format the final comparison table ---\n",
        "    # Convert the list of results into a DataFrame.\n",
        "    results_df = pd.DataFrame(metric_results)\n",
        "\n",
        "    # Set a multi-level index for clear, hierarchical presentation.\n",
        "    results_df.set_index([\"Model\", \"Persona\"], inplace=True)\n",
        "\n",
        "    # Ensure the persona configurations are in the correct order.\n",
        "    persona_order = configs[\"llm_and_prompt_parameters\"][\"persona_configurations\"]\n",
        "    results_df = results_df.reindex(persona_order, level=\"Persona\")\n",
        "\n",
        "    # Announce completion.\n",
        "    print(\"\\n--- Alternative Metric Validation complete. ---\")\n",
        "\n",
        "    # Return the final comparison table.\n",
        "    return results_df\n",
        "\n",
        "\n",
        "def run_comprehensive_robustness_analysis(\n",
        "    analysis_ready_data: Dict[str, Any],\n",
        "    clean_personas_df: pd.DataFrame,\n",
        "    clean_benchmark_df: pd.DataFrame,\n",
        "    study_configurations: Dict[str, Any],\n",
        "    base_output_dir: str,\n",
        "    run_sensitivity_tests: bool = True,\n",
        "    run_alternative_metrics: bool = True,\n",
        "    sample_sizes_to_test: Optional[List[int]] = None,\n",
        "    alternative_trait_sets: Optional[Dict[str, List[str]]] = None,\n",
        "    alternative_prompt_templates: Optional[Dict[str, Dict[str, str]]] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive suite of robustness and validation analyses for\n",
        "    the study's findings.\n",
        "\n",
        "    This high-level function serves as the master controller for Task 8.2. It\n",
        "    integrates and executes two major types of validation:\n",
        "    1.  Multi-Dimensional Sensitivity Analysis (Task 8.2.1): Tests how results\n",
        "        change when key parameters like sample size or persona definitions are\n",
        "        varied. This requires re-running the main simulation pipeline.\n",
        "    2.  Alternative Implementation Validation (Task 8.2.2): Tests if the core\n",
        "        findings hold when using different statistical metrics for comparison.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_data: The dictionary of aggregated data from the main\n",
        "                             pipeline run (output of Task 5).\n",
        "        clean_personas_df: The full, cleaned DataFrame of persona data.\n",
        "        clean_benchmark_df: The full, cleaned DataFrame of benchmark data.\n",
        "        study_configurations: The baseline study configuration dictionary.\n",
        "        base_output_dir: The root directory for storing all analysis outputs.\n",
        "        run_sensitivity_tests: Flag to enable/disable the sensitivity analysis.\n",
        "        run_alternative_metrics: Flag to enable/disable the alternative metric\n",
        "                                 validation.\n",
        "        sample_sizes_to_test: A list of sample sizes for sensitivity testing.\n",
        "        alternative_trait_sets: A dict of alternative persona configurations.\n",
        "        alternative_prompt_templates: A dict of alternative prompt templates.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the structured results of all robustness\n",
        "        analyses performed.\n",
        "    \"\"\"\n",
        "    # Announce the start of the comprehensive analysis.\n",
        "    print(\"\\n--- EXECUTING COMPREHENSIVE ROBUSTNESS ANALYSIS (Task 8.2) ---\")\n",
        "\n",
        "    # Initialize a dictionary to store all robustness analysis results.\n",
        "    robustness_artifacts = {}\n",
        "\n",
        "    # --- Sub-Task 1: Multi-Dimensional Sensitivity Testing ---\n",
        "    # Check if this analysis is enabled.\n",
        "    if run_sensitivity_tests:\n",
        "        # Call the dedicated orchestrator for sensitivity analyses.\n",
        "        sensitivity_results = run_sensitivity_analyses(\n",
        "            personas_df=clean_personas_df,\n",
        "            benchmark_df=clean_benchmark_df,\n",
        "            base_configs=study_configurations,\n",
        "            base_output_dir=base_output_dir,\n",
        "            sample_sizes_to_test=sample_sizes_to_test,\n",
        "            alternative_trait_sets=alternative_trait_sets,\n",
        "            alternative_prompt_templates=alternative_prompt_templates,\n",
        "        )\n",
        "        # Store the results in the main artifacts dictionary.\n",
        "        robustness_artifacts[\"sensitivity_analysis_results\"] = sensitivity_results\n",
        "\n",
        "    # --- Sub-Task 2: Alternative Implementation Validation ---\n",
        "    # Check if this analysis is enabled.\n",
        "    if run_alternative_metrics:\n",
        "        # Call the dedicated function for alternative metric validation.\n",
        "        alternative_metric_results = run_alternative_metric_validation(\n",
        "            analysis_ready_data=analysis_ready_data,\n",
        "            configs=study_configurations,\n",
        "        )\n",
        "        # Store the resulting comparison table.\n",
        "        robustness_artifacts[\"alternative_metric_results\"] = alternative_metric_results\n",
        "        # Display the results for immediate inspection.\n",
        "        print(\"\\n--- Alternative Metric Comparison (Proposer Role) ---\")\n",
        "        from IPython.display import display\n",
        "        display(alternative_metric_results)\n",
        "\n",
        "    # Announce the completion of the task.\n",
        "    print(\"\\n--- Comprehensive Robustness Analysis complete. ---\")\n",
        "\n",
        "    # Return the dictionary containing all generated artifacts.\n",
        "    return robustness_artifacts\n",
        "\n"
      ],
      "metadata": {
        "id": "Drht748Mtl1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Master Orchestrator\n",
        "\n",
        "def run_entire_project(\n",
        "    personas_data_path: str,\n",
        "    benchmark_data_path: str,\n",
        "    study_configurations: Dict[str, Any],\n",
        "    output_dir: str,\n",
        "    force_rerun_simulation: bool = False,\n",
        "    run_robustness_checks: bool = True,\n",
        "    sample_sizes_to_test: Optional[List[int]] = None,\n",
        "    alternative_trait_sets: Optional[Dict[str, List[str]]] = None,\n",
        "    alternative_prompt_templates: Optional[Dict[str, Dict[str, str]]] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire research project, including the main study pipeline\n",
        "    and all subsequent robustness analyses.\n",
        "\n",
        "    This grand master orchestrator serves as the single entry point to\n",
        "    reproduce all findings. It first runs the primary end-to-end pipeline to\n",
        "    generate the main results. It then uses the artifacts from this run to\n",
        "    initiate a comprehensive suite of robustness and sensitivity checks.\n",
        "\n",
        "    Args:\n",
        "        personas_data_path: File path to the raw personas CSV data.\n",
        "        benchmark_data_path: File path to the raw benchmark CSV data.\n",
        "        study_configurations: The main study configuration dictionary.\n",
        "        output_dir: A master directory to save all project outputs.\n",
        "        force_rerun_simulation: If True, forces re-computation of the main\n",
        "                                simulation, ignoring existing checkpoints.\n",
        "        run_robustness_checks: If True, proceeds to run the robustness analyses\n",
        "                               after the main pipeline is complete.\n",
        "        sample_sizes_to_test: A list of sample sizes for sensitivity testing.\n",
        "        alternative_trait_sets: A dict of alternative persona configurations.\n",
        "        alternative_prompt_templates: A dict of alternative prompt templates.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary containing all artifacts from both the main study\n",
        "        and the robustness analyses, providing a complete and auditable record\n",
        "        of the entire project.\n",
        "    \"\"\"\n",
        "    # Announce the start of the entire project workflow.\n",
        "    print(\"=\"*80)\n",
        "    print(\"=== STARTING COMPLETE PROJECT EXECUTION ===\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # --- Step i: Run the Main Study Pipeline ---\n",
        "    # Execute the primary end-to-end pipeline. This function handles all\n",
        "    # steps from data loading through visualization for the main findings.\n",
        "    main_pipeline_artifacts = execute_bias_adjusted_llm_study(\n",
        "        study_configurations=study_configurations,\n",
        "        output_dir=output_dir,\n",
        "        personas_data_path=personas_data_path,\n",
        "        benchmark_data_path=benchmark_data_path,\n",
        "        force_rerun_simulation=force_rerun_simulation,\n",
        "    )\n",
        "\n",
        "    # Initialize the final, top-level results dictionary.\n",
        "    project_results = {\n",
        "        \"main_study_results\": main_pipeline_artifacts,\n",
        "        \"robustness_analysis_results\": None, # Initialize as None\n",
        "    }\n",
        "\n",
        "    # --- Step ii & iii: Run the Comprehensive Robustness Analysis ---\n",
        "    # Proceed to the robustness checks if the flag is enabled.\n",
        "    if run_robustness_checks:\n",
        "        # --- Unpack the necessary artifacts from the main run ---\n",
        "        # These clean dataframes and aggregated results are required inputs\n",
        "        # for the robustness analysis functions.\n",
        "        clean_personas_df = main_pipeline_artifacts[\"clean_personas_df\"]\n",
        "        clean_benchmark_df = main_pipeline_artifacts[\"clean_benchmark_df\"]\n",
        "        analysis_ready_data = main_pipeline_artifacts[\"analysis_ready_data\"]\n",
        "\n",
        "        # Define a dedicated subdirectory for robustness analysis outputs.\n",
        "        robustness_output_dir = os.path.join(output_dir, \"robustness_analysis\")\n",
        "\n",
        "        # --- Execute the robustness analysis orchestrator ---\n",
        "        robustness_artifacts = run_comprehensive_robustness_analysis(\n",
        "            analysis_ready_data=analysis_ready_data,\n",
        "            clean_personas_df=clean_personas_df,\n",
        "            clean_benchmark_df=clean_benchmark_df,\n",
        "            study_configurations=study_configurations,\n",
        "            base_output_dir=robustness_output_dir,\n",
        "            # Pass through the specific test configurations.\n",
        "            sample_sizes_to_test=sample_sizes_to_test,\n",
        "            alternative_trait_sets=alternative_trait_sets,\n",
        "            alternative_prompt_templates=alternative_prompt_templates,\n",
        "        )\n",
        "\n",
        "        # Store the results of the robustness analysis.\n",
        "        project_results[\"robustness_analysis_results\"] = robustness_artifacts\n",
        "\n",
        "    # --- Step iv: Finalization ---\n",
        "    # Announce the successful completion of the entire project.\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"=== COMPLETE PROJECT EXECUTION FINISHED SUCCESSFULLY ===\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Return the final, comprehensive results object.\n",
        "    return project_results\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-k3n28atxSuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "r5WFubHdhK3k"
      }
    }
  ]
}